#!/usr/bin/env python3
"""
Test API Logging Archival System - Complete End-to-End Workflow
Tests the full archival process and then cleans up for production readiness.
"""

import sys
from pathlib import Path
from datetime import datetime

# Legacy transition support pattern (SAME AS WORKING TEST)
repo_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(repo_root))
sys.path.insert(0, str(repo_root / "src"))

from pipelines.utils import db, logger
from src.pipelines.sync_order_list.config_parser import DeltaSyncConfig
from src.pipelines.sync_order_list.api_logging_archiver import APILoggingArchiver

logger = logger.get_logger(__name__)

def main():
    print("üß™ API Logging Archival System - Complete End-to-End Test")
    print("=" * 80)
    
    # Config FIRST
    config_path = str(repo_root / "configs" / "pipelines" / "sync_order_list.toml")
    config = DeltaSyncConfig.from_toml(config_path)
    
    # Database connection using config.db_key
    with db.get_connection(config.db_key) as connection:
        cursor = connection.cursor()
        
        try:
            # Initialize archiver
            archiver = APILoggingArchiver(config)
            
            print("üìä STEP 1: Pre-Archival Analysis")
            print("-" * 50)
            
            # Get current API logging summary
            summary = archiver.get_archival_summary(cursor)
            print(f"   Current API Logs:")
            for status, count in summary.items():
                print(f"     {status}: {count} records")
            
            print(f"\nüì¶ STEP 2: Execute Archival Process")
            print("-" * 50)
            
            # Run archival process
            result = archiver.archive_api_logging_data(cursor)
            
            print(f"   Archival Results:")
            print(f"     Pipeline Run ID: {result['pipeline_run_id']}")
            print(f"     Headers Archived: {result['headers_archived']}")
            print(f"     Lines Archived: {result['lines_archived']}")
            print(f"     Total Records: {result['total_archived']}")
            print(f"     Archive Timestamp: {result['archive_timestamp']}")
            
            if result['total_archived'] > 0:
                print("\n‚úÖ Archival Process Successful!")
                
                print(f"\nüîç STEP 3: Verify Archived Data")
                print("-" * 50)
                
                # Query archived data to verify
                verify_query = """
                SELECT 
                    source,
                    api_status,
                    COUNT(*) as count
                FROM ORDER_LIST_API_LOG 
                WHERE pipeline_run_id = ?
                GROUP BY source, api_status
                ORDER BY source, api_status
                """
                
                cursor.execute(verify_query, (result['pipeline_run_id'],))
                archived_breakdown = cursor.fetchall()
                
                print(f"   Archived Data Breakdown:")
                for row in archived_breakdown:
                    source, status, count = row
                    print(f"     {source} - {status}: {count} records")
                
                print(f"\nüßπ STEP 4: Cleanup for Production")
                print("-" * 50)
                
                # Clean up the test archival for production readiness
                cleanup_query = """
                DELETE FROM ORDER_LIST_API_LOG 
                WHERE pipeline_run_id = ?
                """
                
                cursor.execute(cleanup_query, (result['pipeline_run_id'],))
                deleted_count = cursor.rowcount
                
                print(f"   Cleaned up {deleted_count} test archival records")
                print(f"   Production environment ready ‚úÖ")
                
                connection.commit()
                
                print(f"\nüéØ STEP 5: Final Summary")
                print("-" * 50)
                print(f"   ‚úÖ Archival system tested and validated")
                print(f"   ‚úÖ {result['total_archived']} records successfully archived")
                print(f"   ‚úÖ Data integrity verified")
                print(f"   ‚úÖ Cleanup completed")
                print(f"   ‚úÖ Ready for production deployment")
                
            else:
                print("\n‚ö†Ô∏è  No records to archive (expected if main tables are clean)")
                print("   This is normal for a clean test environment")
            
        except Exception as e:
            print(f"‚ùå Error during archival test: {e}")
            connection.rollback()
            raise
        finally:
            cursor.close()

if __name__ == "__main__":
    main()
