"""
Command Line Interface for ORDER_LIST Delta Sync Pipeline
========================================================
Purpose: Entry point CLI for end-to-end ORDER_LIST ‚Üí Monday.com sync pipeline
Location: src/pipelines/sync_order_list/cli.py
Created: 2025-07-20 (Milestone 2: Business Key Implementation)

This module provides the command line interface for the complete ORDER_LIST delta sync
pipeline. It orchestrates the full workflow from Excel ingestion to Monday.com sync.

Complete Workflow:
1. Excel ingestion (via load_order_list pipeline)
2. Business key generation and canonicalization
3. SQL merge operations (via merge_orchestrator.py)
4. Two-pass Monday.com sync (via monday_sync.py)
5. Comprehensive reporting and validation

Architecture Integration:
- Leverages existing load_order_list pipeline for Excel ingestion
- Uses shared customer utilities for business key resolution
- Orchestrates merge_orchestrator and monday_sync modules
- Provides rich CLI with dry-run support and validation options
"""

import sys
import argparse
import time
import json
from pathlib import Path
from typing import Dict, List, Optional, Any

# Modern import pattern for project utilities
from pipelines.utils import db, logger
from .config_parser import DeltaSyncConfig, load_delta_sync_config
from .merge_orchestrator import MergeOrchestrator
from .monday_sync import MondaySync

# Integration with existing load_order_list pipeline
try:
    from pipelines.load_order_list.extract import OrderListExtractor
    from pipelines.load_order_list.transform import OrderListTransformer
    LOAD_ORDER_LIST_AVAILABLE = True
except ImportError:
    LOAD_ORDER_LIST_AVAILABLE = False

class SyncOrderListCLI:
    """
    Command Line Interface for ORDER_LIST Delta Sync Pipeline
    """
    
    def __init__(self, environment: str = 'dev'):
        """
        Initialize CLI with configuration
        
        Args:
            environment: Configuration environment ('dev', 'prod', or TOML path)
        """
        self.config = load_delta_sync_config(environment)
        self.logger = logger.get_logger(__name__)
        
        # Initialize pipeline components
        config = load_delta_sync_config(environment)
        self.merge_orchestrator = MergeOrchestrator(config)
        self.monday_sync = MondaySync(config)
        
        # Track overall pipeline statistics
        self.pipeline_stats = {
            'start_time': None,
            'end_time': None,
            'total_duration': 0,
            'records_processed': 0,
            'records_synced': 0,
            'operations_completed': []
        }
        
        self.logger.info(f"ORDER_LIST Delta Sync CLI initialized for {environment}")
    
    def execute_complete_pipeline(self, excel_file: Optional[str] = None, dry_run: bool = False, 
                                 validation_only: bool = False) -> Dict[str, Any]:
        """
        Execute complete ORDER_LIST ‚Üí Monday.com sync pipeline
        
        Args:
            excel_file: Path to Excel file (uses latest if None)
            dry_run: If True, validate operations but don't execute
            validation_only: If True, only run validation checks
            
        Returns:
            Complete pipeline execution results
        """
        self.pipeline_stats['start_time'] = time.time()
        
        self.logger.info("üöÄ Starting complete ORDER_LIST Delta Sync Pipeline")
        self.logger.info(f"Environment: {self.config.board_type}")
        self.logger.info(f"Target table: {self.config.target_table}")
        self.logger.info(f"Dry run: {dry_run}")
        
        if validation_only:
            return self._execute_validation_only()
        
        try:
            # Step 1: Excel ingestion (optional - use existing SWP data if not provided)
            if excel_file:
                ingestion_result = self._execute_excel_ingestion(excel_file, dry_run)
                if not ingestion_result['success']:
                    return self._format_failure_result("Excel ingestion failed", ingestion_result)
            else:
                self.logger.info("üìã Skipping Excel ingestion - using existing SWP_ORDER_LIST data")
                ingestion_result = {'success': True, 'records_processed': 0, 'skipped': True}
            
            # Step 2: Merge operations (SWP ‚Üí ORDER_LIST with delta tracking)
            merge_result = self.merge_orchestrator.execute_merge_sequence(dry_run)
            if not merge_result['success']:
                return self._format_failure_result("Merge operations failed", merge_result)
            
            # Step 3: Monday.com sync (two-pass: headers ‚Üí lines)
            sync_result = self.monday_sync.execute_two_pass_sync(dry_run)
            if not sync_result['success']:
                return self._format_failure_result("Monday.com sync failed", sync_result)
            
            # Step 4: Final validation and reporting
            validation_result = self._execute_final_validation()
            
            # Compile complete results
            self.pipeline_stats['end_time'] = time.time()
            self.pipeline_stats['total_duration'] = self.pipeline_stats['end_time'] - self.pipeline_stats['start_time']
            
            result = {
                'success': True,
                'dry_run': dry_run,
                'pipeline_stats': self.pipeline_stats,
                'steps': {
                    'excel_ingestion': ingestion_result,
                    'merge_operations': merge_result,
                    'monday_sync': sync_result,
                    'validation': validation_result
                },
                'summary': self._generate_pipeline_summary()
            }
            
            self.logger.info(f"‚úÖ Complete pipeline finished successfully in {self.pipeline_stats['total_duration']:.2f}s")
            return result
            
        except Exception as e:
            self.logger.exception(f"Fatal pipeline error: {e}")
            return self._format_failure_result("Fatal pipeline error", {'error': str(e)})
    
    def _execute_excel_ingestion(self, excel_file: str, dry_run: bool) -> Dict[str, Any]:
        """
        Execute Excel ingestion using existing load_order_list pipeline
        
        Args:
            excel_file: Path to Excel file to ingest
            dry_run: If True, validate but don't execute
            
        Returns:
            Ingestion results dictionary
        """
        self.logger.info(f"üì• Step 1: Excel Ingestion ({excel_file})")
        
        if not LOAD_ORDER_LIST_AVAILABLE:
            self.logger.warning("load_order_list pipeline not available - using mock ingestion")
            return {
                'success': True,
                'records_processed': 100,
                'duration_seconds': 1.0,
                'mock': True
            }
        
        start_time = time.time()
        
        try:
            if dry_run:
                self.logger.info("üìù DRY RUN: Would ingest Excel file to SWP_ORDER_LIST")
                return {
                    'success': True,
                    'records_processed': 0,
                    'duration_seconds': 0.1,
                    'dry_run': True
                }
            
            # Use existing load_order_list pipeline
            extractor = OrderListExtractor()
            transformer = OrderListTransformer()
            
            # Extract Excel data
            raw_data = extractor.extract_excel(excel_file)
            
            # Transform with business key generation
            transformed_data = transformer.transform_with_business_keys(raw_data)
            
            # Load to SWP_ORDER_LIST
            load_result = extractor.load_to_swp_table(transformed_data)
            
            duration = time.time() - start_time
            
            self.logger.info(f"‚úÖ Excel ingestion completed: {load_result['records']} records in {duration:.2f}s")
            
            return {
                'success': True,
                'records_processed': load_result['records'],
                'duration_seconds': round(duration, 2),
                'file_processed': excel_file
            }
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.exception(f"Excel ingestion failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'duration_seconds': round(duration, 2)
            }
    
    def _execute_validation_only(self) -> Dict[str, Any]:
        """
        Execute validation-only mode (no data processing)
        
        Returns:
            Validation results dictionary
        """
        self.logger.info("üîç Validation-only mode: Checking pipeline readiness")
        
        validation_results = {}
        overall_success = True
        
        # Configuration validation
        config_validation = self._validate_configuration()
        validation_results['configuration'] = config_validation
        if not config_validation['success']:
            overall_success = False
        
        # Database validation
        db_validation = self._validate_database_setup()
        validation_results['database'] = db_validation
        if not db_validation['success']:
            overall_success = False
        
        # Prerequisites validation
        prereq_validation = self.merge_orchestrator.validate_prerequisites()
        validation_results['prerequisites'] = prereq_validation
        if not prereq_validation['success']:
            overall_success = False
        
        # Monday.com integration validation
        monday_validation = self._validate_monday_integration()
        validation_results['monday_integration'] = monday_validation
        if not monday_validation['success']:
            overall_success = False
        
        return {
            'success': overall_success,
            'validation_mode': True,
            'validations': validation_results,
            'summary': f"{'‚úÖ All validations passed' if overall_success else '‚ùå Some validations failed'}"
        }
    
    def _validate_configuration(self) -> Dict[str, Any]:
        """Validate TOML configuration completeness"""
        self.logger.debug("Validating TOML configuration")
        
        try:
            # Check required configuration sections
            required_sections = ['environment', 'database', 'hash', 'monday']
            missing_sections = []
            
            config_dict = self.config._config
            for section in required_sections:
                if section not in config_dict:
                    missing_sections.append(section)
            
            # Validate Monday.com configuration
            monday_valid = self.config.validate_monday_config()
            
            success = len(missing_sections) == 0 and monday_valid
            
            return {
                'success': success,
                'missing_sections': missing_sections,
                'monday_config_valid': monday_valid,
                'target_table': self.config.target_table,
                'board_type': self.config.board_type
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def _validate_database_setup(self) -> Dict[str, Any]:
        """Validate database tables and connections"""
        self.logger.debug("Validating database setup")
        
        try:
            with db.get_connection(self.config.database_connection) as conn:
                cursor = conn.cursor()
                
                validations = {}
                
                # Check target table exists
                target_table = self.config.get_full_table_name('target')
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {target_table}")
                    count = cursor.fetchone()[0]
                    validations['target_table'] = {'exists': True, 'record_count': count}
                except Exception as e:
                    validations['target_table'] = {'exists': False, 'error': str(e)}
                
                # Check SWP table exists
                try:
                    cursor.execute("SELECT COUNT(*) FROM dbo.SWP_ORDER_LIST")
                    swp_count = cursor.fetchone()[0]
                    validations['swp_table'] = {'exists': True, 'record_count': swp_count}
                except Exception as e:
                    validations['swp_table'] = {'exists': False, 'error': str(e)}
                
                success = all(v.get('exists', False) for v in validations.values())
                
                return {
                    'success': success,
                    'connection': 'successful',
                    'tables': validations
                }
                
        except Exception as e:
            return {
                'success': False,
                'connection': 'failed',
                'error': str(e)
            }
    
    def _validate_monday_integration(self) -> Dict[str, Any]:
        """Validate Monday.com integration readiness"""
        self.logger.debug("Validating Monday.com integration")
        
        try:
            # Get sync status
            status = self.monday_sync.get_sync_status()
            
            return {
                'success': not status.get('error'),
                'integration_available': status.get('monday_integration_available', False),
                'board_id': self.config.monday_board_id,
                'sync_status': status
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'integration_available': False
            }
    
    def _execute_final_validation(self) -> Dict[str, Any]:
        """Execute final validation after pipeline completion"""
        self.logger.info("üîç Final Validation: Checking pipeline results")
        
        try:
            # Get updated sync status
            sync_status = self.monday_sync.get_sync_status()
            
            # Count records in various states
            validation_summary = {
                'sync_status': sync_status,
                'validation_time': time.time(),
                'pipeline_successful': True  # Based on previous steps
            }
            
            return {
                'success': True,
                'validation_summary': validation_summary
            }
            
        except Exception as e:
            self.logger.exception(f"Final validation failed: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _format_failure_result(self, message: str, error_details: Dict[str, Any]) -> Dict[str, Any]:
        """Format failure result with consistent structure"""
        self.pipeline_stats['end_time'] = time.time()
        if self.pipeline_stats['start_time']:
            self.pipeline_stats['total_duration'] = self.pipeline_stats['end_time'] - self.pipeline_stats['start_time']
        
        self.logger.error(f"‚ùå Pipeline failed: {message}")
        
        return {
            'success': False,
            'error_message': message,
            'error_details': error_details,
            'pipeline_stats': self.pipeline_stats
        }
    
    def _generate_pipeline_summary(self) -> Dict[str, Any]:
        """Generate comprehensive pipeline summary"""
        return {
            'environment': self.config.board_type,
            'target_table': self.config.target_table,
            'total_duration': round(self.pipeline_stats['total_duration'], 2),
            'operations_completed': len(self.pipeline_stats['operations_completed']),
            'monday_board_id': self.config.monday_board_id,
            'pipeline_version': 'Milestone 2 - Business Key Implementation'
        }


def main():
    """Main CLI entry point"""
    parser = argparse.ArgumentParser(
        description="ORDER_LIST Delta Sync Pipeline - Excel ‚Üí Azure SQL ‚Üí Monday.com",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Complete pipeline with Excel file
  python -m pipelines.sync_order_list.cli --excel orders.xlsx --environment dev
  
  # Dry run validation
  python -m pipelines.sync_order_list.cli --dry-run --environment dev
  
  # Use existing SWP data (skip Excel ingestion)
  python -m pipelines.sync_order_list.cli --environment dev
  
  # Validation only
  python -m pipelines.sync_order_list.cli --validation-only --environment prod
        """
    )
    
    # Primary arguments
    parser.add_argument(
        '--excel', 
        type=str, 
        help='Path to Excel file for ingestion (optional - uses existing SWP data if not provided)'
    )
    
    parser.add_argument(
        '--environment', 
        type=str, 
        default='dev',
        help='Configuration environment: dev, prod, or path to TOML file (default: dev)'
    )
    
    # Execution mode arguments
    parser.add_argument(
        '--dry-run', 
        action='store_true',
        help='Validate operations but do not execute (safe mode)'
    )
    
    parser.add_argument(
        '--validation-only', 
        action='store_true',
        help='Only run validation checks, do not process data'
    )
    
    # Output arguments
    parser.add_argument(
        '--output-json', 
        type=str,
        help='Save results to JSON file'
    )
    
    parser.add_argument(
        '--verbose', 
        action='store_true',
        help='Enable verbose logging'
    )
    
    parser.add_argument(
        '--quiet', 
        action='store_true',
        help='Suppress most output (errors only)'
    )
    
    args = parser.parse_args()
    
    # Configure logging
    log_level = 'DEBUG' if args.verbose else ('ERROR' if args.quiet else 'INFO')
    import logging
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # Initialize CLI
        cli = SyncOrderListCLI(args.environment)
        
        # Execute pipeline
        result = cli.execute_complete_pipeline(
            excel_file=args.excel,
            dry_run=args.dry_run,
            validation_only=args.validation_only
        )
        
        # Output results
        if args.output_json:
            with open(args.output_json, 'w') as f:
                json.dump(result, f, indent=2, default=str)
            print(f"Results saved to {args.output_json}")
        
        if not args.quiet:
            print("\n" + "="*60)
            print("PIPELINE EXECUTION SUMMARY")
            print("="*60)
            print(f"Success: {'‚úÖ Yes' if result['success'] else '‚ùå No'}")
            print(f"Environment: {args.environment}")
            
            if 'pipeline_stats' in result:
                stats = result['pipeline_stats']
                print(f"Duration: {stats.get('total_duration', 0):.2f} seconds")
            
            if 'summary' in result:
                summary = result['summary']
                for key, value in summary.items():
                    print(f"{key.replace('_', ' ').title()}: {value}")
            
            if not result['success'] and 'error_message' in result:
                print(f"\nError: {result['error_message']}")
        
        # Exit with appropriate code
        sys.exit(0 if result['success'] else 1)
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Pipeline interrupted by user")
        sys.exit(2)
    except Exception as e:
        print(f"‚ùå Fatal error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(3)


if __name__ == "__main__":
    main()
