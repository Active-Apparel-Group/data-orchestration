# ğŸš€ Repository Restructure Implementation Plan - ENHANCED

> **Goal**: Transform to modern Python packaging with `src/pipelines` structure, zero production disruption, immediate path fixes, AND eliminate confusing duplications.

## ğŸ¯ Executive Summary

This plan addresses the critical issues:
- âœ… **Path Hacks**: Eliminate `sys.path.append()` throughout codebase
- âœ… **Duplicate Utils**: Consolidate multiple `/utils` folders <important> `pipelines/utils` is primary source of utils. `/utils` only with approval
- âœ… **Modern Packaging**: Implement `pyproject.toml` and `src/` layout
- âœ… **Production Safety**: Zero disruption to Kestra pipelines
- âœ… **Immediate Results**: Working package in 30 minutes
- ğŸ†• **Eliminate Duplications**: Fix confusing `/ddl` and `/integrations` scattered locations
- ğŸ†• **Standardize Structure**: Clear separation of concerns

## ğŸš¨ **Critical Duplication Issues Identified**

### **Problem 1: Duplicate `/ddl` Folders**
- âŒ `db/ddl/` AND `sql/ddl/` - Confusing!
- âœ… **Solution**: Keep only `db/ddl/` (proper schema management)

### **Problem 2: Scattered `/integrations`**  
- âŒ `pipelines/integrations/`, root `/integrations/`, potential `sql/integrations/`
- âœ… **Solution**: Consolidate to `src/pipelines/integrations/` only

### **Problem 3: Mixed Purposes**
- âŒ Schema changes mixed with operational queries
- âœ… **Solution**: Clear separation - `db/` for schema evolution, `sql/` for operations

## ğŸ—‚ï¸ **Target Structure (Final State)**

```text
data-orchestration/
â”œâ”€ src/                          # NEW - Modern Python package
â”‚  â””â”€ pipelines/                 # Main package (NOT data-orchestration!)
â”‚     â”œâ”€ __init__.py            # Package root
â”‚     â”œâ”€ utils/                 # Consolidated utilities
â”‚     â”‚  â”œâ”€ __init__.py
â”‚     â”‚  â”œâ”€ db.py              # from pipelines/utils/db_helper.py
â”‚     â”‚  â”œâ”€ logger.py          # from pipelines/utils/logger_helper.py
â”‚     â”‚  â”œâ”€ config.py          # configuration management
â”‚     â”‚  â””â”€ schema.py          # schema utilities
â”‚     â”œâ”€ load_order_list/      # Pipeline modules (from existing)
â”‚     â”‚  â”œâ”€ __init__.py
â”‚     â”‚  â”œâ”€ extract.py
â”‚     â”‚  â”œâ”€ transform.py
â”‚     â”‚  â””â”€ load.py
â”‚     â”œâ”€ load_cms/           # Customer Master Schedule
â”‚     â”‚  â”œâ”€ __init__.py
â”‚     â”‚  â””â”€ etl.py
â”‚     â”œâ”€ update/             # Update operations
â”‚     â”‚  â”œâ”€ __init__.py
â”‚     â”‚  â””â”€ boards.py
â”‚     â”œâ”€ transform/          # Data transformations
â”‚     â”‚  â”œâ”€ __init__.py
â”‚     â”‚  â””â”€ processors.py
â”‚     â”œâ”€ ingestion/          # Data ingestion
â”‚     â”‚  â”œâ”€ __init__.py
â”‚     â”‚  â””â”€ sources.py
â”‚     â”œâ”€ powerbi/           # PowerBI integrations
â”‚     â”‚  â”œâ”€ __init__.py
â”‚     â”‚  â””â”€ refresh.py
â”‚     â””â”€ integrations/         # External APIs
â”‚        â”œâ”€ __init__.py
â”‚        â””â”€ azure/
â”‚
â”œâ”€ pipelines/                   # KEEP - Production Kestra (unchanged!)
â”‚  â”œâ”€ scripts/
â”‚  â”œâ”€ utils/                   # Keep during transition
â”‚  â””â”€ workflows/
â”‚
â”œâ”€ sql/                        # MOVED from src/sql
â”‚  â”œâ”€ ddl/
â”‚  â”œâ”€ graphql/
â”‚  â”œâ”€ mappings/
â”‚  â””â”€ migrations/
â”‚
â”œâ”€ configs/                    # Environment configs
â”œâ”€ workflows/                  # Kestra YAML
â”œâ”€ tests/                      # All tests
â”œâ”€ tools/                      # PowerShell tools
â”œâ”€ docs/                       # Documentation
â”œâ”€ db/                         # Keep existing
â”œâ”€ integrations/               # Keep existing
â”‚
â”œâ”€ __legacy/                   # Archive of old structure
â”‚  â”œâ”€ src/                    # Original src/ folder
â”‚  â”œâ”€ utils/                  # Original utils/
â”‚  â””â”€ templates/              # Original templates/
â”‚
â”œâ”€ pyproject.toml             # Modern Python packaging
â”œâ”€ requirements.txt
â””â”€ README.md
```

---

## ğŸš¨ Phase 0: Emergency Setup (30 Minutes)

### Pre-Action Planning âœ…
- **Goal**: Working `pyproject.toml` and package structure with zero breaking changes
- **Context**: Move legacy folders, create new structure, establish imports
- **Approach**: Archive old, create new, compatibility layer for transition

### Step 0.1: Archive Legacy Folders (5 minutes)
```powershell
# Create archive and move problematic folders
mkdir __legacy
move src __legacy\src
move utils __legacy\utils
move templates __legacy\templates
```

### Step 0.2: Create New Package Structure (5 minutes)
```powershell
# Create modern src/ layout
mkdir src\pipelines\utils
mkdir src\pipelines\load_order_list
mkdir src\pipelines\load_cms
mkdir src\pipelines\update
mkdir src\pipelines\transform
mkdir src\pipelines\ingestion
mkdir src\pipelines\powerbi
mkdir src\pipelines\integrations

# Create __init__.py files
echo. > src\pipelines\__init__.py
echo. > src\pipelines\utils\__init__.py
echo. > src\pipelines\load_order_list\__init__.py
echo. > src\pipelines\load_cms\__init__.py
echo. > src\pipelines\update\__init__.py
echo. > src\pipelines\transform\__init__.py
echo. > src\pipelines\ingestion\__init__.py
echo. > src\pipelines\powerbi\__init__.py
echo. > src\pipelines\integrations\__init__.py
```

### Step 0.3: Move SQL Folder (2 minutes)
```powershell
# Extract SQL from legacy src
move __legacy\src\sql .\sql
```

### Step 0.4: Create pyproject.toml (3 minutes)
```toml
[build-system]
requires = ["setuptools>=64", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "pipelines"
version = "0.1.0"
description = "AAG Data Orchestration Pipelines"
authors = [{name = "Active Apparel Group"}]
requires-python = ">=3.10"
dependencies = [
    "pandas>=2.0",
    "pyodbc",
    "pyyaml",
    "requests",
    "python-dotenv",
    "jinja2",
    "azure-storage-blob",
    "azure-identity",
    "tomli",
]

[tool.setuptools.packages.find]
where = ["src"]
include = ["pipelines*"]

[tool.setuptools.package-data]
"pipelines" = [
    "**/*.sql", "**/*.yaml", "**/*.yml", 
    "**/*.json", "**/*.graphql", "**/*.toml"
]

[project.scripts]
pipelines-order-list = "pipelines.order_list.main:main"
pipelines-monday-sync = "pipelines.monday_boards.sync:main"
```

### Step 0.5: Create Compatibility Layer (10 minutes)

Create utils with legacy import compatibility:

**src/pipelines/utils/db.py**:
```python
"""Database utilities - migrated from pipelines/utils/db_helper.py"""
import sys
from pathlib import Path

# Source from pipelines/utils (NOT root utils - that's outdated)
pipelines_utils_path = Path(__file__).parent.parent.parent.parent / "pipelines" / "utils"
if pipelines_utils_path.exists():
    sys.path.insert(0, str(pipelines_utils_path))
    
    try:
        from db_helper import *
        # Re-export for new package structure
        get_connection = get_connection
        load_config = load_config
    except ImportError as e:
        print(f"Warning: Could not import from pipelines/utils/db_helper: {e}")

# TODO: Replace with native implementation
```

**src/pipelines/utils/logger.py**:
```python
"""Logger utilities - migrated from pipelines/utils/logger_helper.py"""
import sys
from pathlib import Path

# Source from pipelines/utils (NOT root utils - that's outdated)
pipelines_utils_path = Path(__file__).parent.parent.parent.parent / "pipelines" / "utils"
if pipelines_utils_path.exists():
    sys.path.insert(0, str(pipelines_utils_path))
    
    try:
        from logger_helper import *
        get_logger = get_logger
    except ImportError as e:
        print(f"Warning: Could not import from pipelines/utils/logger_helper: {e}")

# TODO: Replace with native implementation
```

**src/pipelines/utils/config.py**:
```python
"""Configuration utilities"""
import sys
from pathlib import Path

# Source from pipelines/utils (NOT root utils - that's outdated)
pipelines_utils_path = Path(__file__).parent.parent.parent.parent / "pipelines" / "utils"
if pipelines_utils_path.exists():
    sys.path.insert(0, str(pipelines_utils_path))
    
    try:
        from db_helper import load_config
    except ImportError as e:
        print(f"Warning: Could not import config from pipelines/utils: {e}")

# TODO: Create dedicated config module
```

### Step 0.6: Create Emergency Tests (5 minutes)

**tests/test_emergency_package.py**:
```python
"""Emergency test - validate package works immediately"""
import subprocess
import sys
from pathlib import Path

def test_package_install():
    """Test package installs in editable mode"""
    print("ğŸ§ª Testing package installation...")
    result = subprocess.run([
        sys.executable, "-m", "pip", "install", "-e", "."
    ], capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"âŒ Install failed: {result.stderr}")
        return False
    
    print("âœ… Package installed successfully")
    return True

def test_critical_imports():
    """Test critical imports work"""
    print("ğŸ§ª Testing critical imports...")
    
    imports = [
        "pipelines",
        "pipelines.utils",
        "pipelines.utils.db", 
        "pipelines.utils.logger",
        "pipelines.utils.config",
    ]
    
    failures = []
    for module in imports:
        try:
            __import__(module)
            print(f"âœ… {module}")
        except ImportError as e:
            print(f"âŒ {module}: {e}")
            failures.append((module, e))
    
    return len(failures) == 0

def test_legacy_functions():
    """Test legacy function compatibility"""
    print("ğŸ§ª Testing legacy function access...")
    
    try:
        from pipelines.utils.db import get_connection, load_config
        from pipelines.utils.logger import get_logger
        
        # Test they're callable
        config = load_config()
        logger = get_logger(__name__)
        
        print("âœ… Legacy functions accessible")
        return True
    except Exception as e:
        print(f"âŒ Legacy compatibility failed: {e}")
        return False

def main():
    """Run emergency validation"""
    print("ğŸš¨ EMERGENCY PACKAGE VALIDATION")
    print("=" * 50)
    
    tests = [
        ("Package Install", test_package_install),
        ("Critical Imports", test_critical_imports),
        ("Legacy Functions", test_legacy_functions),
    ]
    
    all_passed = True
    for name, test_func in tests:
        print(f"\n{name}:")
        print("-" * 30)
        passed = test_func()
        if not passed:
            all_passed = False
    
    status = "âœ… ALL TESTS PASSED" if all_passed else "âŒ SOME TESTS FAILED" 
    print(f"\n{status}")
    
    if all_passed:
        print("\nğŸ‰ Package ready for use!")
        print("Next: Import using 'from pipelines.utils import db, logger'")
    
    return all_passed

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
```

---

## ï¿½ **Phase 1: ENHANCED - Duplication Cleanup & Consolidation** âš¡ **NEXT**

### Pre-Action Planning
- **Goal**: Eliminate confusing duplications, standardize folder structure, maintain production safety
- **Context**: Multiple DDL locations, scattered integrations, mixed purposes causing confusion
- **Approach**: Systematic consolidation with clear separation of concerns

### **Phase 1.1: DDL Consolidation (10 minutes)**

**Current Mess**:
```
â”œâ”€â”€ db/ddl/           # âœ… Schema management (keep)
â”œâ”€â”€ sql/ddl/          # âŒ Confusing duplicate (remove)
```

**Target State**:
```
â”œâ”€â”€ db/ddl/           # âœ… ONLY location for DDL
â”œâ”€â”€ sql/operations/   # âœ… Operational queries only
```

**Actions**:
```powershell
# 1. Move sql/ddl content to db/ddl with conflict resolution
robocopy sql\ddl db\ddl /MIR /XO  # Skip older files

# 2. Remove empty sql/ddl
rmdir sql\ddl /s /q

# 3. Create operations folder for SQL queries
mkdir sql\operations
```

### **Phase 1.2: Integrations Consolidation (15 minutes)**

**Current Scatter**:
```
â”œâ”€â”€ integrations/            # âŒ Root level (move)
â”œâ”€â”€ pipelines/integrations/  # âŒ Old location (move)
â”œâ”€â”€ sql/integrations/        # âŒ Wrong place (move)
```

**Target State**:
```
â”œâ”€â”€ src/pipelines/integrations/  # âœ… ONLY location
    â”œâ”€â”€ monday/
    â”œâ”€â”€ powerbi/
    â”œâ”€â”€ azure/
    â””â”€â”€ external/
```

**Actions**:
```powershell
# 1. Consolidate all integration code
mkdir src\pipelines\integrations\monday
mkdir src\pipelines\integrations\powerbi
mkdir src\pipelines\integrations\azure
mkdir src\pipelines\integrations\external

# 2. Move from scattered locations
robocopy integrations src\pipelines\integrations\external /MIR
robocopy pipelines\integrations src\pipelines\integrations\monday /MIR
robocopy sql\integrations src\pipelines\integrations\external /MIR

# 3. Create __init__.py files
echo. > src\pipelines\integrations\__init__.py
echo. > src\pipelines\integrations\monday\__init__.py
echo. > src\pipelines\integrations\powerbi\__init__.py
echo. > src\pipelines\integrations\azure\__init__.py
echo. > src\pipelines\integrations\external\__init__.py

# 4. Clean up old locations
rmdir integrations /s /q
rmdir pipelines\integrations /s /q
rmdir sql\integrations /s /q
```

### **Phase 1.3: Clear Purpose Separation (10 minutes)**

**Database Structure Clarification**:
```
â”œâ”€â”€ db/               # ğŸ“ Schema Evolution & Management
    â”œâ”€â”€ ddl/          # âœ… CREATE, ALTER, DROP statements
    â”œâ”€â”€ migrations/   # âœ… Version-controlled schema changes
    â””â”€â”€ tests/        # âœ… Schema validation tests

â”œâ”€â”€ sql/              # ğŸ“ Operations & Business Logic  
    â”œâ”€â”€ operations/   # âœ… SELECT queries, procedures
    â”œâ”€â”€ transformations/ # âœ… ETL queries
    â”œâ”€â”€ utility/      # âœ… Admin and maintenance queries
    â””â”€â”€ graphql/      # âœ… Monday.com API templates
```

**Actions**:
```powershell
# 1. Organize sql/ by purpose
mkdir sql\operations
move sql\*.sql sql\operations\  # Move loose SQL files

# 2. Update documentation references
# (Will be done in documentation phase)
```

### **Phase 1.4: Migration Validation (10 minutes)**

**Create validation script**:
```powershell
# tools/validate_consolidation.py
python tools\validate_consolidation.py --check-ddl --check-integrations
```

**Verification checklist**:
- [ ] DDL only in `db/ddl/` 
- [ ] Integrations only in `src/pipelines/integrations/`
- [ ] No orphaned files
- [ ] All imports still working
- [ ] Documentation updated

### **Post-Action Review**
- **Show original goals**: âœ… Eliminate duplications, âœ… Standardize structure, âœ… Maintain production safety
- **Mark completed items**: All Phase 1 consolidation steps
- **Document any failures**: Migration conflicts (to be handled in validation)
- **Preventative action**: Enhanced validation script for future changes

---

## ï¿½ğŸ“‹ Phase 1: Import Migration (Week 1)

### Migration Script
**tools/migrate_imports.py**:
```python
"""Migrate Python files to use new package imports"""
import re
from pathlib import Path
import argparse

MIGRATION_PATTERNS = [
    # Replace sys.path hacks
    (r'sys\.path\.insert\(0, str\(.*?utils.*?\)\)\s*\nimport db_helper',
     'from pipelines.utils import db'),
    (r'sys\.path\.insert\(0, str\(.*?utils.*?\)\)\s*\nimport logger_helper', 
     'from pipelines.utils import logger'),
    
    # Replace direct imports  
    (r'^import db_helper as db$', 'from pipelines.utils import db'),
    (r'^import logger_helper$', 'from pipelines.utils import logger'),
    
    # Replace function calls
    (r'db_helper\.', 'db.'),
    (r'logger_helper\.', 'logger.'),
]

def migrate_file(file_path: Path, dry_run: bool = True):
    """Migrate imports in a single file"""
    if file_path.suffix != '.py':
        return False
        
    content = file_path.read_text(encoding='utf-8', errors='ignore')
    original = content
    
    for pattern, replacement in MIGRATION_PATTERNS:
        content = re.sub(pattern, replacement, content, flags=re.MULTILINE)
    
    if content != original:
        if not dry_run:
            file_path.write_text(content, encoding='utf-8')
            print(f"âœ… Migrated: {file_path}")
        else:
            print(f"ğŸ“ Would migrate: {file_path}")
        return True
    
    return False

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--apply', action='store_true', help='Apply changes')
    parser.add_argument('--path', default='.', help='Path to scan')
    args = parser.parse_args()
    
    root_path = Path(args.path)
    skip_dirs = {'__pycache__', '.git', '__legacy__', '.venv'}
    
    migrated = 0
    for py_file in root_path.rglob('*.py'):
        if any(part in skip_dirs for part in py_file.parts):
            continue
        if migrate_file(py_file, dry_run=not args.apply):
            migrated += 1
    
    print(f"\n{'Migrated' if args.apply else 'Would migrate'} {migrated} files")

if __name__ == "__main__":
    main()
```

---

## ğŸ¯ Success Metrics

### Phase 0 Complete (TODAY):
- [ ] `__legacy/` created with old folders archived
- [ ] New `src/pipelines/` structure established
- [ ] `pyproject.toml` working with `pip install -e .`
- [ ] Emergency tests passing
- [ ] Legacy imports still functional

### **Current Status Summary**

| Phase | Status | Duration | Key Achievement |
|-------|---------|----------|-----------------|
| **Phase 0** | âœ… **COMPLETE** | 30 min | Working modern package structure |
| **Enhanced Phase 1** | ğŸ“‹ **READY** | 45 min | Duplication cleanup & consolidation |
| **Phase 2** | ğŸ“‹ Planned | 30 min | Import migration tools |
| **Phase 3** | ğŸ“‹ Planned | 15 min | Legacy deprecation |

### **Phase 0 Achievements**:
- âœ… Modern `src/pipelines/` package structure created
- âœ… Full `pyproject.toml` with all dependencies consolidated
- âœ… Compatibility layer: `src/pipelines/utils/` (db.py, logger.py, config.py)
- âœ… Legacy code safely archived in `__legacy/`
- âœ… Emergency test suite validates all imports
- âœ… Package installs: `pip install -e .`
- âœ… Zero breaking changes to production

### Phase 1 Complete (Week 1):
- [ ] DDL consolidation (only in `db/ddl/`)
- [ ] Integrations consolidation (only in `src/pipelines/integrations/`)
- [ ] Clear purpose separation (`db/` vs `sql/`)
- [ ] Migration validation tools
- [ ] Legacy imports still functional

### Phase 2 Complete (Week 1):
- [ ] Import migration tools ready
- [ ] Selected scripts using package imports
- [ ] Comprehensive test coverage
- [ ] No breaking changes

---

## ğŸš€ **NEXT STEPS - Execute Enhanced Phase 1**

**Immediate Actions** (next 45 minutes):

1. **Execute Enhanced Phase 1.1: DDL Consolidation** (10 min)
   - Move `sql/ddl/*` â†’ `db/ddl/`
   - Remove duplicate DDL locations
   - Create `sql/operations/` for queries

2. **Execute Enhanced Phase 1.2: Integrations Consolidation** (15 min)  
   - Consolidate all integration code to `src/pipelines/integrations/`
   - Clean up scattered integration folders
   - Update import paths

3. **Execute Enhanced Phase 1.3: Purpose Separation** (10 min)
   - Organize `sql/` by operational purpose
   - Update documentation references
   - Validate folder structure

4. **Execute Enhanced Phase 1.4: Validation** (10 min)
   - Run consolidation validation script
   - Test all imports still working
   - Document changes

**Why Enhanced Phase 1 Now?**
- âœ… **Immediate Impact**: Eliminates confusing duplications
- âœ… **Low Risk**: File moves with safety checks  
- âœ… **High Value**: Clear, logical structure
- âœ… **Foundation**: Sets up clean base for import migration

**After Phase 1**: Repository will have clean, logical structure with zero duplications and clear purpose separation, ready for import migration tools in Phase 2.

---

## ğŸ¯ Long-term Success Metrics

This approach ensures:
- âœ… **Immediate modern packaging** with working `pyproject.toml`
- âœ… **Zero production disruption** via compatibility layer
- âœ… **Clean `src/pipelines` structure** (not data-orchestration)
- âœ… **Eliminated duplications** with clear separation of concerns
- âœ… **Gradual migration path** with full backward compatibility
- âœ… **Comprehensive testing** at each phase

**Ready to execute Enhanced Phase 1 now!**
