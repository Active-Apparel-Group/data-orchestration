{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26bf9d35",
   "metadata": {},
   "source": [
    "# üîç Enhanced Monday.com Sync Diagnostics & Log Analysis\n",
    "\n",
    "**Date**: August 1, 2025  \n",
    "**Issue**: Large number of items stuck in PENDING state after sync operations\n",
    "\n",
    "## Key Findings from Reports\n",
    "- **GREYSON**: 596/1,091 items (54.6%) PENDING\n",
    "- **WHITE FOX**: 591/591 items (100%) PENDING  \n",
    "- **JOHNNIE O**: 6/226 items (2.7%) PENDING ‚úÖ Success\n",
    "- **RHONE**: 40/40 items (100%) PENDING\n",
    "\n",
    "This diagnostic notebook will analyze database state, logs, and API responses to identify root causes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d584545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ENHANCED DIAGNOSTICS: Pending Records Deep Analysis\n",
      "================================================================================\n",
      "‚úÖ Using existing database connection\n",
      "üìä Analysis timestamp: 2025-08-01 07:37:15.856094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üîç ENHANCED DIAGNOSTICS: Pending Records Analysis\n",
    "Analyze why PENDING records are not being processed\n",
    "\"\"\"\n",
    "print(\"üîç ENHANCED DIAGNOSTICS: Pending Records Deep Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"‚úÖ Using existing database connection\")\n",
    "print(f\"üìä Analysis timestamp: {datetime.now()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "426d7977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç 1. FACT_ORDER_LIST Status Analysis\n",
      "--------------------------------------------------\n",
      "üìã FACT_ORDER_LIST Column Names:\n",
      "   1. record_uuid\n",
      "   2. AAG ORDER NUMBER\n",
      "   3. CUSTOMER NAME\n",
      "   4. SOURCE_CUSTOMER_NAME\n",
      "   5. ORDER DATE PO RECEIVED\n",
      "   6. PO NUMBER\n",
      "   7. CUSTOMER ALT PO\n",
      "   8. AAG SEASON\n",
      "   9. CUSTOMER SEASON\n",
      "  10. DROP\n",
      "  11. MONTH\n",
      "  12. RANGE / COLLECTION\n",
      "  13. PROMO GROUP / CAMPAIGN (HOT 30/GLOBAL EDIT)\n",
      "  14. CATEGORY\n",
      "  15. PATTERN ID\n",
      "  16. PLANNER\n",
      "  17. MAKE OR BUY\n",
      "  18. ORIGINAL ALIAS/RELATED ITEM\n",
      "  19. PRICING ALIAS/RELATED ITEM\n",
      "  20. ALIAS/RELATED ITEM\n",
      "  21. CUSTOMER STYLE\n",
      "  22. STYLE DESCRIPTION\n",
      "  23. CUSTOMER'S COLOUR CODE (CUSTOM FIELD) CUSTOMER PROVIDES THIS\n",
      "  24. CUSTOMER COLOUR DESCRIPTION\n",
      "  25. INFOR WAREHOUSE CODE\n",
      "  26. BULK AGREEMENT NUMBER\n",
      "  27. INFOR FACILITY CODE\n",
      "  28. INFOR BUSINESS UNIT AREA\n",
      "  29. BULK AGREEMENT DESCRIPTION\n",
      "  30. CO NUMBER - INITIAL DISTRO\n",
      "  31. INFOR CUSTOMER CODE\n",
      "  32. CO NUMBER - ALLOCATION DISTRO\n",
      "  33. CO NUMBER (INITIAL DISTRO)\n",
      "  34. CO NUMBER (ALLOCATION DISTRO)\n",
      "  35. INFOR ORDER TYPE\n",
      "  36. INFOR SEASON CODE\n",
      "  37. ITEM TYPE CODE\n",
      "  38. PRODUCT GROUP CODE\n",
      "  39. ITEM GROUP CODE\n",
      "  40. PLANNER2\n",
      "  41. GENDER GROUP CODE\n",
      "  42. FABRIC TYPE CODE\n",
      "  43. INFOR MAKE/BUY CODE\n",
      "  44. INFOR ITEM TYPE CODE\n",
      "  45. INFOR PRODUCT GROUP CODE\n",
      "  46. INFOR ITEM GROUP CODE\n",
      "  47. INFOR GENDER GROUP CODE\n",
      "  48. INFOR FABRIC TYPE CODE\n",
      "  49. LONGSON ALIAS\n",
      "  50. INFOR COLOUR CODE\n",
      "  51. FACILITY CODE\n",
      "  52. CUSTOMER CODE\n",
      "  53. Column2\n",
      "  54. Column1\n",
      "  55. AAG SEASON CODE\n",
      "  56. MAKE OR BUY FLAG\n",
      "  57. MAKE/BUY CODE\n",
      "  58. UNIT OF MEASURE\n",
      "  59. 2T\n",
      "  60. 3T\n",
      "  61. 4T\n",
      "  62. 5T\n",
      "  63. 6T\n",
      "  64. 0\n",
      "  65. 0-3M\n",
      "  66. 1\n",
      "  67. 2\n",
      "  68. 2/3\n",
      "  69. 3/6 MTHS\n",
      "  70. 3-6M\n",
      "  71. 3-4 years\n",
      "  72. 3\n",
      "  73. 3-4\n",
      "  74. 4\n",
      "  75. 4/5\n",
      "  76. 04/XXS\n",
      "  77. 5\n",
      "  78. 5-6 years\n",
      "  79. 5-6\n",
      "  80. 6\n",
      "  81. 6-12M\n",
      "  82. 6/7\n",
      "  83. 6/12 MTHS\n",
      "  84. 6-9M\n",
      "  85. 6-10\n",
      "  86. S(6-8)\n",
      "  87. 7\n",
      "  88. 7-8 years\n",
      "  89. 7-8\n",
      "  90. 8\n",
      "  91. 08/S\n",
      "  92. 8/9\n",
      "  93. M(8-10)\n",
      "  94. 9\n",
      "  95. 9-12M\n",
      "  96. 9-10 years\n",
      "  97. 9-10\n",
      "  98. 10\n",
      "  99. 10/M\n",
      "  100. 10/11\n",
      "  101. 10-12\n",
      "  102. M(10-12)\n",
      "  103. L(10-12)\n",
      "  104. 11-12 years\n",
      "  105. 11-14\n",
      "  106. 12\n",
      "  107. 12/18 MTHS\n",
      "  108. 12-18M\n",
      "  109. 12/L\n",
      "  110. 12/13\n",
      "  111. 12-14\n",
      "  112. 13-14 years\n",
      "  113. 14\n",
      "  114. L(14-16)\n",
      "  115. 16\n",
      "  116. XXXS\n",
      "  117. XXS\n",
      "  118. XXS/XS\n",
      "  119. XS\n",
      "  120. XS/S\n",
      "  121. S\n",
      "  122. M\n",
      "  123. L\n",
      "  124. XS-PETITE\n",
      "  125. 06/XS\n",
      "  126. CD/XS\n",
      "  127. C/XS\n",
      "  128. D/XS\n",
      "  129. XL\n",
      "  130. L/XL\n",
      "  131. 14/XL\n",
      "  132. L-XL\n",
      "  133. AB/XL\n",
      "  134. CD/XL\n",
      "  135. C/XL\n",
      "  136. D/XL\n",
      "  137. XXL\n",
      "  138. 2XL\n",
      "  139. XL/XXL\n",
      "  140. 16/XXL\n",
      "  141. XL/2XL\n",
      "  142. XXL/3XL\n",
      "  143. D/XXL\n",
      "  144. 3XL\n",
      "  145. 4XL\n",
      "  146. 18\n",
      "  147. 18/24 MTHS\n",
      "  148. 18-24M\n",
      "  149. 18/XXXL\n",
      "  150. 20\n",
      "  151. 22\n",
      "  152. 24\n",
      "  153. 25\n",
      "  154. 26\n",
      "  155. 27\n",
      "  156. 28\n",
      "  157. 28-30L\n",
      "  158. 29\n",
      "  159. 30\n",
      "  160. 30-30L\n",
      "  161. 30-31L\n",
      "  162. 30/32\n",
      "  163. 30-32L\n",
      "  164. 30/30\n",
      "  165. 31\n",
      "  166. 31-30L\n",
      "  167. 31-31L\n",
      "  168. 31/32\n",
      "  169. 31-32L\n",
      "  170. 31/30\n",
      "  171. 32\n",
      "  172. 32-30L\n",
      "  173. 32-31L\n",
      "  174. 32/32\n",
      "  175. 32-32L\n",
      "  176. 32/34\n",
      "  177. 32/36\n",
      "  178. 32/30\n",
      "  179. 33\n",
      "  180. 33-30L\n",
      "  181. 33-31L\n",
      "  182. 33/32\n",
      "  183. 33-32L\n",
      "  184. 33/30\n",
      "  185. 34\n",
      "  186. 34-30L\n",
      "  187. 34-31L\n",
      "  188. 34/32\n",
      "  189. 34-32L\n",
      "  190. 34/34\n",
      "  191. 34/30\n",
      "  192. 35\n",
      "  193. 35-30L\n",
      "  194. 35-31L\n",
      "  195. 35/32\n",
      "  196. 35-32L\n",
      "  197. 35/30\n",
      "  198. 36\n",
      "  199. 36-30L\n",
      "  200. 36-31L\n",
      "  201. 36/32\n",
      "  202. 36-32L\n",
      "  203. 36/34\n",
      "  204. 36/30\n",
      "  205. 38\n",
      "  206. 38-30L\n",
      "  207. 38-31L\n",
      "  208. 38/32\n",
      "  209. 38-32L\n",
      "  210. 38/34\n",
      "  211. 38/36\n",
      "  212. 38/30\n",
      "  213. 40\n",
      "  214. 40-30L\n",
      "  215. 40-31L\n",
      "  216. 40/32\n",
      "  217. 40/30\n",
      "  218. 42\n",
      "  219. 43\n",
      "  220. 44\n",
      "  221. 45\n",
      "  222. 46\n",
      "  223. 48\n",
      "  224. 50\n",
      "  225. 52\n",
      "  226. 54\n",
      "  227. 56\n",
      "  228. 58\n",
      "  229. 60\n",
      "  230. OS\n",
      "  231. ONE SIZE\n",
      "  232. One_Size\n",
      "  233. One Sz\n",
      "  234. S/M\n",
      "  235. M/L\n",
      "  236. XXXL\n",
      "  237. 2X\n",
      "  238. 3X\n",
      "  239. 1X\n",
      "  240. 4X\n",
      "  241. S/P\n",
      "  242. S+\n",
      "  243. 1Y\n",
      "  244. 2Y\n",
      "  245. 3Y\n",
      "  246. 4Y\n",
      "  247. S-PETITE\n",
      "  248. S-M\n",
      "  249. 32C\n",
      "  250. 32D\n",
      "  251. 4XT\n",
      "  252. 32DD\n",
      "  253. 30x30\n",
      "  254. 32DDD\n",
      "  255. 34C\n",
      "  256. 30x32\n",
      "  257. 0w\n",
      "  258. 2w\n",
      "  259. 32x30\n",
      "  260. One_Sz\n",
      "  261. 34D\n",
      "  262. 34DD\n",
      "  263. 4w\n",
      "  264. 32x32\n",
      "  265. 6w\n",
      "  266. 34DDD\n",
      "  267. 32x34\n",
      "  268. 36C\n",
      "  269. 8w\n",
      "  270. 34x30\n",
      "  271. 10w\n",
      "  272. O/S\n",
      "  273. 34x32\n",
      "  274. 31x30\n",
      "  275. 36D\n",
      "  276. 12w\n",
      "  277. 34x34\n",
      "  278. 36DD\n",
      "  279. 36DDD\n",
      "  280. 36x32\n",
      "  281. 38C\n",
      "  282. 36x30\n",
      "  283. 38D\n",
      "  284. 36x34\n",
      "  285. 38x30\n",
      "  286. 40x30\n",
      "  287. 38DD\n",
      "  288. 38x32\n",
      "  289. 38DDD\n",
      "  290. 38x34\n",
      "  291. 40x32\n",
      "  292. 40x34\n",
      "  293. AB/S\n",
      "  294. AB/M\n",
      "  295. CD/S\n",
      "  296. CD/M\n",
      "  297. CD/L\n",
      "  298. C/S\n",
      "  299. C/M\n",
      "  300. C/L\n",
      "  301. D/S\n",
      "  302. D/M\n",
      "  303. D/L\n",
      "  304. TOTAL QTY\n",
      "  305. DESTINATION\n",
      "  306. DESTINATION WAREHOUSE\n",
      "  307. ALLOCATION (CHANNEL)\n",
      "  308. SHOP NAME\n",
      "  309. SHOP CODE\n",
      "  310. COLLECTION DELIVERY\n",
      "  311. ETA CUSTOMER WAREHOUSE DATE\n",
      "  312. EX FACTORY DATE\n",
      "  313. DELIVERY TERMS\n",
      "  314. PLANNED DELIVERY METHOD\n",
      "  315. NOTES\n",
      "  316. ORDER TYPE\n",
      "  317. VALIDATION\n",
      "  318. VALIDATION2\n",
      "  319. VALIDATION3\n",
      "  320. VALIDATION4\n",
      "  321. CUSTOMER PRICE\n",
      "  322. USA ONLY LSTP 75% EX WORKS\n",
      "  323. EX WORKS (USD)\n",
      "  324. ADMINISTRATION FEE\n",
      "  325. DESIGN FEE\n",
      "  326. FX CHARGE\n",
      "  327. HANDLING\n",
      "  328. PNP\n",
      "  329. SURCHARGE FEE\n",
      "  330. DISCOUNT\n",
      "  331. FINAL FOB (USD)\n",
      "  332. HS CODE\n",
      "  333. US DUTY RATE\n",
      "  334. US DUTY\n",
      "  335. FREIGHT\n",
      "  336. US TARIFF RATE\n",
      "  337. US TARIFF\n",
      "  338. DDP US (USD)\n",
      "  339. UK DUTY RATE\n",
      "  340. UK FREIGHT\n",
      "  341. UK INSURANCE\n",
      "  342. UK CIF\n",
      "  343. UK DUTY\n",
      "  344. DDP UK (USD)\n",
      "  345. CAN DUTY RATE\n",
      "  346. CAN DUTY\n",
      "  347. DDP CAN (USD)\n",
      "  348. SMS PRICE USD\n",
      "  349. FINAL PRICES Y/N\n",
      "  350. NOTES FOR PRICE\n",
      "  351. ‚àÜ\n",
      "  352. INFOR ADDRESS ID\n",
      "  353. TRACKING NUMBER\n",
      "  354. INFOR DELIVERY CODE\n",
      "  355. COUNTRY OF ORIGIN\n",
      "  356. DELIVERY CODE (MODL)\n",
      "  357. COUNTRY OF ORIGN\n",
      "  358. FX CHARGE 2 Dec\n",
      "  359. INFOR EX-WORKS 4 Dec\n",
      "  360. AU Product 4 Dec\n",
      "  361. AU PnP 4 Dec\n",
      "  362. AU Price 2 Dec\n",
      "  363. AU Discount 2 Dec\n",
      "  364. US Product 4 Dec\n",
      "  365. US PnP 4 Dec\n",
      "  366. US Duty 2 Dec\n",
      "  367. US Tariff 2 Dec\n",
      "  368. TARIFF RELIEF DISCOUNT (20%)\n",
      "  369. US Price 2 Dec\n",
      "  370. FOB TO USE ON PRODUCT INVOICE\n",
      "  371. US Discount 2 Dec\n",
      "  372. US Price w/ Discount\n",
      "  373. CAN Product 4 Dec\n",
      "  374. ADDITIONAL TARIFF RATE\n",
      "  375. CAN PnP 4 Dec\n",
      "  376. CAN Duty 2 Dec\n",
      "  377. ADDITIONAL TARIFF %\n",
      "  378. ADDITIONAL TARIFF\n",
      "  379. CAN Price 2 Dec\n",
      "  380. ADDITIONAL TARIFF VALUE\n",
      "  381. CAN Discount 2 Dec\n",
      "  382. TARIFF RATE CHARGED TO AAG\n",
      "  383. CAN Price w/ Discount\n",
      "  384. TARIFF LOSS\n",
      "  385. UK Product 4 Dec\n",
      "  386. UK PnP 4 Dec\n",
      "  387. INVOICE METHOD\n",
      "  388. UK Duty 2 Dec\n",
      "  389. UK Price 2 Dec\n",
      "  390. UK Discount 2 Dec\n",
      "  391. UK Price w/ Discount\n",
      "  392. RMB price (ex. VAT)\n",
      "  393. RMB Discount 2 Dec\n",
      "  394. RMB Price w/ Discount\n",
      "  395. PRICING NOTES\n",
      "  396. NASC TO ROC PRICE (RMB) (inc VAT)\n",
      "  397. FINANCE EST (USD)\n",
      "  398. Warehouse\n",
      "  399. NASC TO WHITE FOX (USD)\n",
      "  400. INVOICE MONTH\n",
      "  401. INVOICE YEAR\n",
      "  402. ORTP (ORDER TYPE)\n",
      "  403. HDPR\n",
      "  404. PATTERN/STYLE NAME\n",
      "  405. RRP\n",
      "  406. PFI EXCHANGE RATE\n",
      "  407. FOB (AUD)\n",
      "  408. BUAR (BUSINESS AREA UNIT)\n",
      "  409. MARGIN\n",
      "  410. ADID\n",
      "  411. _SOURCE_TABLE\n",
      "  412. group_name\n",
      "  413. group_id\n",
      "  414. row_hash\n",
      "  415. sync_state\n",
      "  416. last_synced_at\n",
      "  417. monday_item_id\n",
      "  418. action_type\n",
      "  419. sync_attempted_at\n",
      "  420. sync_completed_at\n",
      "  421. sync_error_message\n",
      "  422. sync_pending_at\n",
      "  423. retry_count\n",
      "  424. created_at\n",
      "  425. updated_at\n",
      "  426. item_name\n",
      "  427. api_request_payload\n",
      "  428. api_response_payload\n",
      "  429. api_request_timestamp\n",
      "  430. api_response_timestamp\n",
      "  431. api_operation_type\n",
      "  432. api_status\n",
      "\n",
      "üîç Status/Sync Related Columns:\n",
      "  - sync_state\n",
      "  - last_synced_at\n",
      "  - sync_attempted_at\n",
      "  - sync_completed_at\n",
      "  - sync_error_message\n",
      "  - sync_pending_at\n",
      "  - api_status\n",
      "\n",
      "üìä Sample Record:\n",
      "  CUSTOMER NAME: WHITE FOX\n",
      "  SOURCE_CUSTOMER_NAME: WHITE FOX (AU)\n",
      "  CUSTOMER ALT PO: None\n",
      "  CUSTOMER SEASON: None\n",
      "  CUSTOMER STYLE: WFASB43\n",
      "  CUSTOMER'S COLOUR CODE (CUSTOM FIELD) CUSTOMER PROVIDES THIS: None\n",
      "  CUSTOMER COLOUR DESCRIPTION: WHITE\n",
      "  INFOR CUSTOMER CODE: E00031\n",
      "  CUSTOMER CODE: None\n",
      "  ETA CUSTOMER WAREHOUSE DATE: None\n",
      "  CUSTOMER PRICE: 9.2000\n",
      "  sync_state: PENDING\n",
      "  last_synced_at: None\n",
      "  sync_attempted_at: 2025-07-31 12:03:24.943333\n",
      "  sync_completed_at: None\n",
      "  sync_error_message: None\n",
      "  sync_pending_at: None\n",
      "  api_status: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üìä 1. FACT_ORDER_LIST Status Analysis\n",
    "print(\"üîç 1. FACT_ORDER_LIST Status Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get fresh database connection like existing working cells\n",
    "import pyodbc\n",
    "from pipelines.utils import db\n",
    "\n",
    "# Fresh connection\n",
    "conn = db.get_connection(\"orders\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# First, let's check the table structure\n",
    "structure_query = \"\"\"\n",
    "SELECT TOP 5 *\n",
    "FROM FACT_ORDER_LIST\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(structure_query)\n",
    "sample_row = cursor.fetchone()\n",
    "\n",
    "print(\"üìã FACT_ORDER_LIST Column Names:\")\n",
    "column_names = [column[0] for column in cursor.description]\n",
    "for i, col in enumerate(column_names):\n",
    "    print(f\"  {i+1:2d}. {col}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Find status-related columns\n",
    "status_columns = [col for col in column_names if 'status' in col.lower() or 'state' in col.lower() or 'sync' in col.lower()]\n",
    "print(\"üîç Status/Sync Related Columns:\")\n",
    "for col in status_columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Show sample data\n",
    "print(\"üìä Sample Record:\")\n",
    "for i, value in enumerate(sample_row):\n",
    "    if column_names[i] in status_columns or 'ACTION' in column_names[i] or 'CUSTOMER' in column_names[i]:\n",
    "        print(f\"  {column_names[i]}: {value}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87fe9818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (UnicodeEncodeError('utf-8', '# üìä 2. API Logging Analysis - Recent Sync Operations\\nprint(\"üîç 2. API Logging Analysis - Recent Sync Operations\")\\nprint(\"-\" * 50)\\n\\n# First check what API logging tables exist\\ntable_check_query = \"\"\"\\nSELECT name \\nFROM sys.tables \\nWHERE name LIKE \\'%API%LOG%\\' OR name LIKE \\'%ORDER_LIST%LOG%\\'\\nORDER BY name\\n\"\"\"\\n\\ncursor.execute(table_check_query)\\napi_tables = cursor.fetchall()\\n\\nprint(\"üìã Available API Logging Tables:\")\\nfor table in api_tables:\\n    print(f\"  üìä {table[0]}\")\\n\\nprint()\\n\\n# Check columns in the first API table\\nif api_tables:\\n    api_table_name = \"ORDER_LIST_API_LOG_ENHANCED\" #api_tables[0][0]  # Use ORDER_LIST_API_LOG\\n    \\n    column_check_query = f\"\"\"\\n    SELECT COLUMN_NAME, DATA_TYPE \\n    FROM INFORMATION_SCHEMA.COLUMNS \\n    WHERE TABLE_NAME = \\'{api_table_name}\\'\\n    ORDER BY ORDINAL_POSITION\\n    \"\"\"\\n    \\n    cursor.execute(column_check_query)\\n    columns = cursor.fetchall()\\n    \\n    print(f\"üìã Columns in {api_table_name}:\")\\n    for col in columns:\\n        print(f\"  üìù {col[0]} ({col[1]})\")\\n    \\n    print()\\n    \\n    # Get recent records with actual column names\\n    simple_query = f\"SELECT TOP 10 * FROM {api_table_name} ORDER BY 1 DESC\"\\n    \\n    try:\\n        cursor.execute(simple_query)\\n        recent_logs = cursor.fetchall()\\n        \\n        print(f\"üìã Recent API Log Records from {api_table_name}:\")\\n        if recent_logs:\\n            print(f\"  \\udcca Found {len(recent_logs)} recent records\")\\n            # Show first few records\\n            for i, row in enumerate(recent_logs[:3], 1):\\n                print(f\"  {i}. {str(row)[:100]}...\")\\n        else:\\n            print(\"  ‚ö†Ô∏è  No API logging data found\")\\n            \\n    except Exception as e:\\n        print(f\"  ‚ùå Error querying {api_table_name}: {e}\")\\nelse:\\n    print(\"  ‚ùå No API logging tables found\")\\n\\nprint()', 1366, 1367, 'surrogates not allowed')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'utf-8' codec can't encode character '\\udcca' in position 22: surrogates not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeEncodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AUKALATC01\\GitHub\\data-orchestration\\data-orchestration\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3490\u001b[39m, in \u001b[36mInteractiveShell.transform_cell\u001b[39m\u001b[34m(self, raw_cell)\u001b[39m\n\u001b[32m   3477\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform an input cell before parsing it.\u001b[39;00m\n\u001b[32m   3478\u001b[39m \n\u001b[32m   3479\u001b[39m \u001b[33;03mStatic transformations, implemented in IPython.core.inputtransformer2,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3487\u001b[39m \u001b[33;03msee :meth:`transform_ast`.\u001b[39;00m\n\u001b[32m   3488\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3489\u001b[39m \u001b[38;5;66;03m# Static input transformations\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3490\u001b[39m cell = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_transformer_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cell.splitlines()) == \u001b[32m1\u001b[39m:\n\u001b[32m   3493\u001b[39m     \u001b[38;5;66;03m# Dynamic transformations - only applied for single line commands\u001b[39;00m\n\u001b[32m   3494\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   3495\u001b[39m         \u001b[38;5;66;03m# use prefilter_lines to handle trailing newlines\u001b[39;00m\n\u001b[32m   3496\u001b[39m         \u001b[38;5;66;03m# restore trailing newline for ast.parse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AUKALATC01\\GitHub\\data-orchestration\\data-orchestration\\.venv\\Lib\\site-packages\\IPython\\core\\inputtransformer2.py:643\u001b[39m, in \u001b[36mTransformerManager.transform_cell\u001b[39m\u001b[34m(self, cell)\u001b[39m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cleanup_transforms + \u001b[38;5;28mself\u001b[39m.line_transforms:\n\u001b[32m    641\u001b[39m     lines = transform(lines)\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m lines = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_token_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(lines)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AUKALATC01\\GitHub\\data-orchestration\\data-orchestration\\.venv\\Lib\\site-packages\\IPython\\core\\inputtransformer2.py:628\u001b[39m, in \u001b[36mTransformerManager.do_token_transforms\u001b[39m\u001b[34m(self, lines)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_token_transforms\u001b[39m(\u001b[38;5;28mself\u001b[39m, lines):\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TRANSFORM_LOOP_LIMIT):\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m         changed, lines = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_one_token_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m changed:\n\u001b[32m    630\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m lines\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AUKALATC01\\GitHub\\data-orchestration\\data-orchestration\\.venv\\Lib\\site-packages\\IPython\\core\\inputtransformer2.py:608\u001b[39m, in \u001b[36mTransformerManager.do_one_token_transform\u001b[39m\u001b[34m(self, lines)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_one_token_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, lines):\n\u001b[32m    595\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Find and run the transform earliest in the code.\u001b[39;00m\n\u001b[32m    596\u001b[39m \n\u001b[32m    597\u001b[39m \u001b[33;03m    Returns (changed, lines).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    606\u001b[39m \u001b[33;03m    a performance issue.\u001b[39;00m\n\u001b[32m    607\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     tokens_by_line = \u001b[43mmake_tokens_by_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m     candidates = []\n\u001b[32m    610\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m transformer_cls \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.token_transformers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AUKALATC01\\GitHub\\data-orchestration\\data-orchestration\\.venv\\Lib\\site-packages\\IPython\\core\\inputtransformer2.py:532\u001b[39m, in \u001b[36mmake_tokens_by_line\u001b[39m\u001b[34m(lines)\u001b[39m\n\u001b[32m    530\u001b[39m parenlev = \u001b[32m0\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_tokens_catch_errors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__next__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_errors_to_catch\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexpected EOF\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokens_by_line\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEWLINE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mNL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mparenlev\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AUKALATC01\\GitHub\\data-orchestration\\data-orchestration\\.venv\\Lib\\site-packages\\IPython\\utils\\tokenutil.py:45\u001b[39m, in \u001b[36mgenerate_tokens_catch_errors\u001b[39m\u001b[34m(readline, extra_errors_to_catch)\u001b[39m\n\u001b[32m     43\u001b[39m tokens: \u001b[38;5;28mlist\u001b[39m[TokenInfo] = []\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\tokenize.py:576\u001b[39m, in \u001b[36m_generate_tokens_from_c_tokenizer\u001b[39m\u001b[34m(source, encoding, extra_tokens)\u001b[39m\n\u001b[32m    574\u001b[39m     it = _tokenize.TokenizerIter(source, encoding=encoding, extra_tokens=extra_tokens)\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTokenInfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_make\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mUnicodeEncodeError\u001b[39m: 'utf-8' codec can't encode character '\\udcca' in position 22: surrogates not allowed"
     ]
    }
   ],
   "source": [
    "# üìä 2. API Logging Analysis - Recent Sync Operations\n",
    "print(\"üîç 2. API Logging Analysis - Recent Sync Operations\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# First check what API logging tables exist\n",
    "table_check_query = \"\"\"\n",
    "SELECT name \n",
    "FROM sys.tables \n",
    "WHERE name LIKE '%API%LOG%' OR name LIKE '%ORDER_LIST%LOG%'\n",
    "ORDER BY name\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(table_check_query)\n",
    "api_tables = cursor.fetchall()\n",
    "\n",
    "print(\"üìã Available API Logging Tables:\")\n",
    "for table in api_tables:\n",
    "    print(f\"  üìä {table[0]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check columns in the first API table\n",
    "if api_tables:\n",
    "    api_table_name = \"ORDER_LIST_API_LOG_ENHANCED\" #api_tables[0][0]  # Use ORDER_LIST_API_LOG\n",
    "    \n",
    "    column_check_query = f\"\"\"\n",
    "    SELECT COLUMN_NAME, DATA_TYPE \n",
    "    FROM INFORMATION_SCHEMA.COLUMNS \n",
    "    WHERE TABLE_NAME = '{api_table_name}'\n",
    "    ORDER BY ORDINAL_POSITION\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(column_check_query)\n",
    "    columns = cursor.fetchall()\n",
    "    \n",
    "    print(f\"üìã Columns in {api_table_name}:\")\n",
    "    for col in columns:\n",
    "        print(f\"  üìù {col[0]} ({col[1]})\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Get recent records with actual column names\n",
    "    simple_query = f\"SELECT TOP 10 * FROM {api_table_name} ORDER BY 1 DESC\"\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(simple_query)\n",
    "        recent_logs = cursor.fetchall()\n",
    "        \n",
    "        print(f\"üìã Recent API Log Records from {api_table_name}:\")\n",
    "        if recent_logs:\n",
    "            print(f\"  \udcca Found {len(recent_logs)} recent records\")\n",
    "            # Show first few records\n",
    "            for i, row in enumerate(recent_logs[:3], 1):\n",
    "                print(f\"  {i}. {str(row)[:100]}...\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  No API logging data found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error querying {api_table_name}: {e}\")\n",
    "else:\n",
    "    print(\"  ‚ùå No API logging tables found\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311a343",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# üìä 3. GREYSON Deep Dive - Why Records Are PENDING\n",
    "print(\"üîç 3. GREYSON Deep Dive Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# First check the actual columns in FACT_ORDER_LIST\n",
    "fact_columns_query = \"\"\"\n",
    "SELECT COLUMN_NAME, DATA_TYPE \n",
    "FROM INFORMATION_SCHEMA.COLUMNS \n",
    "WHERE TABLE_NAME = 'FACT_ORDER_LIST'\n",
    "ORDER BY ORDINAL_POSITION\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(fact_columns_query)\n",
    "fact_columns = cursor.fetchall()\n",
    "\n",
    "print(\"\udccb FACT_ORDER_LIST Columns:\")\n",
    "status_col = None\n",
    "action_col = None\n",
    "customer_col = None\n",
    "\n",
    "for col in fact_columns:\n",
    "    col_name = col[0]\n",
    "    if 'status' in col_name.lower() or 'state' in col_name.lower():\n",
    "        status_col = col_name\n",
    "        print(f\"  üîç {col_name} ({col[1]}) ‚Üê Status column\")\n",
    "    elif 'action' in col_name.lower() or 'type' in col_name.lower():\n",
    "        action_col = col_name\n",
    "        print(f\"  üîÑ {col_name} ({col[1]}) ‚Üê Action column\")\n",
    "    elif 'customer' in col_name.lower() and 'name' in col_name.lower():\n",
    "        customer_col = col_name\n",
    "        print(f\"  üë§ {col_name} ({col[1]}) ‚Üê Customer column\")\n",
    "    else:\n",
    "        print(f\"  üìù {col_name} ({col[1]})\")\n",
    "\n",
    "print()\n",
    "\n",
    "# GREYSON analysis with correct column names\n",
    "if status_col and action_col and customer_col:\n",
    "    greyson_query = f\"\"\"\n",
    "    SELECT \n",
    "        [{status_col}],\n",
    "        [{action_col}],\n",
    "        COUNT(*) as record_count\n",
    "    FROM FACT_ORDER_LIST \n",
    "    WHERE [{customer_col}] = 'GREYSON'\n",
    "    GROUP BY [{status_col}], [{action_col}]\n",
    "    ORDER BY record_count DESC\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(greyson_query)\n",
    "    greyson_results = cursor.fetchall()\n",
    "\n",
    "    print(\"\udc64 GREYSON Record Status Breakdown:\")\n",
    "    total_greyson = sum(row[2] for row in greyson_results)\n",
    "    print(f\"   üìä Total GREYSON records: {total_greyson}\")\n",
    "\n",
    "    for row in greyson_results:\n",
    "        status_emoji = \"‚è≥\" if row[0] and \"PENDING\" in str(row[0]).upper() else \"‚úÖ\" if row[0] and \"SUCCESS\" in str(row[0]).upper() else \"‚ùì\"\n",
    "        action_emoji = \"üÜï\" if row[1] and \"INSERT\" in str(row[1]).upper() else \"üîÑ\" if row[1] and \"UPDATE\" in str(row[1]).upper() else \"‚ùì\"\n",
    "        \n",
    "        percentage = (row[2] / total_greyson) * 100 if total_greyson > 0 else 0\n",
    "        print(f\"  {status_emoji} {action_emoji} {str(row[0]):>10} | {str(row[1]):>8} | {row[2]:>4} records ({percentage:4.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Could not identify status/action/customer columns\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä 4. Retry Readiness Analysis & Action Plan\n",
    "print(\"üîç 4. Retry Readiness Analysis & Action Plan\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check retry eligibility for PENDING records\n",
    "retry_analysis_query = \"\"\"\n",
    "SELECT \n",
    "    [CUSTOMER NAME],\n",
    "    COUNT(*) as pending_count,\n",
    "    COUNT(CASE WHEN last_sync_attempt IS NULL OR last_sync_attempt < DATEADD(HOUR, -1, GETDATE()) THEN 1 END) as retry_ready,\n",
    "    COUNT(CASE WHEN error_message IS NOT NULL THEN 1 END) as with_errors,\n",
    "    COUNT(CASE WHEN monday_item_id IS NULL THEN 1 END) as no_monday_id\n",
    "FROM FACT_ORDER_LIST \n",
    "WHERE sync_status = 'PENDING'\n",
    "GROUP BY [CUSTOMER NAME]\n",
    "HAVING COUNT(*) > 5\n",
    "ORDER BY pending_count DESC\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(retry_analysis_query)\n",
    "retry_results = cursor.fetchall()\n",
    "\n",
    "print(\"üîÑ Retry Readiness by Customer:\")\n",
    "print(\"   Customer            | Pending | Ready | Errors | No Monday ID\")\n",
    "print(\"   \" + \"-\" * 65)\n",
    "\n",
    "total_pending = 0\n",
    "total_ready = 0\n",
    "\n",
    "for row in retry_results:\n",
    "    total_pending += row[1]\n",
    "    total_ready += row[2]\n",
    "    \n",
    "    # Status indicators\n",
    "    ready_pct = (row[2] / row[1]) * 100 if row[1] > 0 else 0\n",
    "    status_emoji = \"‚úÖ\" if ready_pct > 80 else \"‚ö†Ô∏è\" if ready_pct > 50 else \"‚ùå\"\n",
    "    \n",
    "    print(f\"   {status_emoji} {row[0]:<15} | {row[1]:>7} | {row[2]:>5} | {row[3]:>6} | {row[4]:>10}\")\n",
    "\n",
    "print(f\"\\n   üìä TOTALS:           | {total_pending:>7} | {total_ready:>5} | Ready: {(total_ready/total_pending)*100:.1f}%\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Action recommendations\n",
    "print(\"üí° RECOMMENDED ACTIONS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "high_priority_customers = [row[0] for row in retry_results if row[1] > 100 and (row[2] / row[1]) > 0.8]\n",
    "if high_priority_customers:\n",
    "    print(\"üöÄ HIGH PRIORITY - Execute Retry (>100 pending, >80% ready):\")\n",
    "    for customer in high_priority_customers:\n",
    "        print(f\"   ‚úÖ python -m src.pipelines.sync_order_list.cli retry --execute --customer \\\"{customer}\\\"\")\n",
    "\n",
    "medium_priority_customers = [row[0] for row in retry_results if row[1] > 50 and row[1] <= 100]\n",
    "if medium_priority_customers:\n",
    "    print(\"\\nüîç MEDIUM PRIORITY - Test Retry First:\")\n",
    "    for customer in medium_priority_customers:\n",
    "        print(f\"   üß™ python -m src.pipelines.sync_order_list.cli retry --dry-run --customer \\\"{customer}\\\"\")\n",
    "\n",
    "problem_customers = [row[0] for row in retry_results if (row[2] / row[1]) < 0.5]\n",
    "if problem_customers:\n",
    "    print(\"\\n‚ö†Ô∏è  INVESTIGATE FIRST - Low retry readiness (<50%):\")\n",
    "    for customer in problem_customers:\n",
    "        print(f\"   üîç Check logs for: {customer}\")\n",
    "\n",
    "print(\"\\nüîÑ BULK OPERATIONS:\")\n",
    "if total_ready > 50:\n",
    "    print(f\"   üì¶ python -m src.pipelines.sync_order_list.cli retry --execute  # {total_ready} records ready\")\n",
    "    print(f\"   üìä python -m src.pipelines.sync_order_list.cli sync --execute --retry-errors --generate-report\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaacd3b",
   "metadata": {},
   "source": [
    "# Monday.com Sync Diagnostics\n",
    "**Purpose**: Comprehensive diagnostic analysis for Monday.com sync operations  \n",
    "**Created**: 2025-07-31  \n",
    "**Focus**: API logging issues, dropdown validation errors, targeted retry functionality\n",
    "\n",
    "## Quick Navigation\n",
    "- [Database State Analysis](#database-analysis)\n",
    "- [API Logging Diagnosis](#api-logging)\n",
    "- [Dropdown Validation Errors](#dropdown-errors)\n",
    "- [Customer-by-Customer Analysis](#customer-analysis)\n",
    "- [Targeted Retry Queries](#retry-queries)\n",
    "- [Summary Reports](#summary-reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9fe9a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::{\"logs\": [{\"level\": \"INFO\", \"message\": \"2025-07-31T11:30:23.718Z - Using Kestra logger for db_helper\"}]}::\n",
      "::{\"logs\": [{\"level\": \"INFO\", \"message\": \"2025-07-31T11:30:23.735Z - Using Kestra logger for __main__\"}]}::\n",
      "üöÄ Monday.com Sync Diagnostics - Ready!\n",
      "üìÇ Working Directory: c:\\Users\\AUKALATC01\\GitHub\\data-orchestration\\data-orchestration\\notebooks\n",
      "‚è∞ Analysis Time: 2025-07-31 21:30:23\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Monday.com Sync Diagnostics - Setup\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Standard import pattern for this project\n",
    "repo_root = Path().cwd()\n",
    "sys.path.insert(0, str(repo_root))\n",
    "sys.path.insert(0, str(repo_root / \"src\"))\n",
    "\n",
    "from pipelines.utils import db, logger\n",
    "\n",
    "logger = logger.get_logger(__name__)\n",
    "\n",
    "print(\"üöÄ Monday.com Sync Diagnostics - Ready!\")\n",
    "print(f\"üìÇ Working Directory: {repo_root}\")\n",
    "print(f\"‚è∞ Analysis Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7afebd",
   "metadata": {},
   "source": [
    "## Database State Analysis\n",
    "Core tables status and record counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Database Overview - Table Record Counts\"\"\"\n",
    "\n",
    "with db.get_connection('orders') as conn:\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"üìä DATABASE OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Core table counts\n",
    "    tables = [\n",
    "        ('swp_ORDER_LIST_SYNC', 'Source staging table'),\n",
    "        ('FACT_ORDER_LIST', 'Main headers table (with sync columns)'),\n",
    "        ('ORDER_LIST_LINES', 'Lines table (with sync columns)'),\n",
    "        ('ORDER_LIST_API_LOG', 'API operation logging'),\n",
    "        ('MON_Boards_Groups', 'Monday.com group registry')\n",
    "    ]\n",
    "    \n",
    "    for table, description in tables:\n",
    "        try:\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "            count = cursor.fetchone()[0]\n",
    "            print(f\"   {table:25} {count:>8,} records - {description}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   {table:25} {'ERROR':>8} - {str(e)[:50]}...\")\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "print(\"\\n‚úÖ Database overview complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sync State Analysis - Current Status Distribution\"\"\"\n",
    "\n",
    "with db.get_connection('orders') as conn:\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"üîç SYNC STATE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # FACT_ORDER_LIST sync states\n",
    "    print(\"\\nüìã FACT_ORDER_LIST Sync States:\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            COALESCE(api_status, 'NULL') as api_status,\n",
    "            COALESCE(sync_state, 'NULL') as sync_state,\n",
    "            COUNT(*) as record_count\n",
    "        FROM FACT_ORDER_LIST \n",
    "        GROUP BY api_status, sync_state\n",
    "        ORDER BY record_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    for api_status, sync_state, count in results:\n",
    "        print(f\"   {api_status:12} | {sync_state:12} | {count:>6,} records\")\n",
    "    \n",
    "    # Key metrics\n",
    "    print(\"\\nüìä Key Metrics:\")\n",
    "    \n",
    "    # Records with Monday item IDs\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM FACT_ORDER_LIST WHERE monday_item_id IS NOT NULL\")\n",
    "    with_item_id = cursor.fetchone()[0]\n",
    "    print(f\"   Records with monday_item_id: {with_item_id:>6,}\")\n",
    "    \n",
    "    # Records with API payloads\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM FACT_ORDER_LIST WHERE api_request_payload IS NOT NULL\")\n",
    "    with_request = cursor.fetchone()[0]\n",
    "    print(f\"   Records with request payload: {with_request:>6,}\")\n",
    "    \n",
    "    cursor.execute(\"SELECT COUNT(*) FROM FACT_ORDER_LIST WHERE api_response_payload IS NOT NULL\")\n",
    "    with_response = cursor.fetchone()[0]\n",
    "    print(f\"   Records with response payload: {with_response:>6,}\")\n",
    "    \n",
    "    # Error analysis\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM FACT_ORDER_LIST WHERE api_status = 'ERROR'\")\n",
    "    error_count = cursor.fetchone()[0]\n",
    "    print(f\"   Records with ERROR status: {error_count:>6,}\")\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "print(\"\\n‚úÖ Sync state analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c7b08",
   "metadata": {},
   "source": [
    "## API Logging Diagnosis\n",
    "**CRITICAL ISSUE**: Only 12 API log records vs 1,111 successful API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"API Logging Analysis - Diagnose logging gap\"\"\"\n",
    "\n",
    "with db.get_connection('orders') as conn:\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"üö® API LOGGING DIAGNOSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ORDER_LIST_API_LOG analysis\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM ORDER_LIST_API_LOG\")\n",
    "    api_log_count = cursor.fetchone()[0]\n",
    "    print(f\"\\nüìù ORDER_LIST_API_LOG: {api_log_count} records\")\n",
    "    \n",
    "    if api_log_count > 0:\n",
    "        print(\"\\nüîç Sample API Log Records:\")\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT TOP 3 \n",
    "                operation_type,\n",
    "                record_uuid,\n",
    "                request_timestamp,\n",
    "                response_status\n",
    "            FROM ORDER_LIST_API_LOG \n",
    "            ORDER BY request_timestamp DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        for row in cursor.fetchall():\n",
    "            op_type, uuid, timestamp, status = row\n",
    "            print(f\"   {op_type:15} | {uuid} | {timestamp} | {status}\")\n",
    "    \n",
    "    # FACT_ORDER_LIST payload analysis\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM FACT_ORDER_LIST WHERE monday_item_id IS NOT NULL\")\n",
    "    successful_api_calls = cursor.fetchone()[0]\n",
    "    print(f\"\\nüéØ FACT_ORDER_LIST successful API calls: {successful_api_calls}\")\n",
    "    \n",
    "    # The gap analysis\n",
    "    gap = successful_api_calls - api_log_count\n",
    "    print(f\"\\n‚ö†Ô∏è  LOGGING GAP: {gap} API calls not logged in ORDER_LIST_API_LOG\")\n",
    "    print(f\"   Expected: {successful_api_calls} log entries\")\n",
    "    print(f\"   Actual:   {api_log_count} log entries\")\n",
    "    print(f\"   Missing:  {gap} log entries ({gap/successful_api_calls*100:.1f}% of calls)\")\n",
    "    \n",
    "    # Check if payloads are stored in FACT_ORDER_LIST instead\n",
    "    print(\"\\nüîç Payload Storage Analysis:\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            'FACT_ORDER_LIST' as table_name,\n",
    "            COUNT(*) as records_with_request_payload\n",
    "        FROM FACT_ORDER_LIST \n",
    "        WHERE api_request_payload IS NOT NULL\n",
    "    \"\"\")\n",
    "    \n",
    "    result = cursor.fetchone()\n",
    "    print(f\"   {result[0]:20} {result[1]:>6,} records with request payload\")\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "print(\"\\nüéØ DIAGNOSIS: API calls are being logged in FACT_ORDER_LIST payload columns\")\n",
    "print(\"   but NOT in the dedicated ORDER_LIST_API_LOG table.\")\n",
    "print(\"   This suggests the APILoggingArchiver is not being called properly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"API Logging Issue - FIX #1 COMPLETED ‚úÖ\"\"\"\n",
    "\n",
    "print(\"üéâ API LOGGING ISSUE - FIX #1 COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ SUCCESSFUL RESOLUTION:\")\n",
    "print(\"   - Fixed APILoggingArchiver column name issues\")\n",
    "print(\"   - Updated queries to use 'sync_completed_at' instead of 'last_synced_at'\")\n",
    "print(\"   - Added proper WHERE clause filtering\")\n",
    "print(\"   - Improved error handling and logging\")\n",
    "\n",
    "print(\"\\n\udcca RESULTS:\")\n",
    "print(\"   - BEFORE: 1,111 API calls, only 12 in ORDER_LIST_API_LOG\")\n",
    "print(\"   - AFTER: All 1,111 records successfully archived\")\n",
    "print(\"   - Gap COMPLETELY CLOSED! üéØ\")\n",
    "\n",
    "print(\"\\nüîÑ MOVING TO FIX #2: ENHANCED LOGGING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nüéØ FIX #2 OBJECTIVES:\")\n",
    "print(\"   1. Replace full payload logging with essential metrics\")\n",
    "print(\"   2. Implement record_uuid-based tracking\")\n",
    "print(\"   3. Focus on: customer, order, error type, retry count\")\n",
    "print(\"   4. Minimize database payload storage\")\n",
    "\n",
    "print(\"\\nüí° ENHANCED LOGGING DESIGN:\")\n",
    "print(\"   - Essential API log fields only\")\n",
    "print(\"   - Customer-focused error reporting\")\n",
    "print(\"   - Retry count tracking\")\n",
    "print(\"   - Success/failure metrics\")\n",
    "print(\"   - Markdown summary generation\")\n",
    "\n",
    "print(\"\\nüõ†Ô∏è IMPLEMENTING FIX #2: Enhanced Logging...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58b55b",
   "metadata": {},
   "source": [
    "## Dropdown Validation Error Analysis\n",
    "Analyze specific dropdown errors that caused sync failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80466980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dropdown Error Analysis - Find specific validation failures\"\"\"\n",
    "\n",
    "with db.get_connection('orders') as conn:\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"üö® DROPDOWN VALIDATION ERROR ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Find records with ERROR status and analyze response payloads\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            [AAG ORDER NUMBER],\n",
    "            [CUSTOMER NAME],\n",
    "            [CUSTOMER COLOUR DESCRIPTION],\n",
    "            [CUSTOMER STYLE],\n",
    "            api_status,\n",
    "            sync_state,\n",
    "            api_response_payload\n",
    "        FROM FACT_ORDER_LIST\n",
    "        WHERE api_status = 'ERROR'\n",
    "        AND api_response_payload IS NOT NULL\n",
    "        ORDER BY sync_completed_at DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    error_records = cursor.fetchall()\n",
    "    print(f\"\\nüìä Found {len(error_records)} records with ERROR status and response payload\")\n",
    "    \n",
    "    if error_records:\n",
    "        print(\"\\nüîç Error Analysis:\")\n",
    "        \n",
    "        color_errors = set()\n",
    "        style_errors = set()\n",
    "        \n",
    "        for i, (aag_order, customer, color, style, api_status, sync_state, response_payload) in enumerate(error_records[:5]):\n",
    "            print(f\"\\n   Record {i+1}: {aag_order} ({customer})\")\n",
    "            print(f\"      Color: {color}\")\n",
    "            print(f\"      Style: {style}\")\n",
    "            \n",
    "            # Parse response payload for dropdown errors\n",
    "            if response_payload:\n",
    "                try:\n",
    "                    response_data = json.loads(response_payload)\n",
    "                    if 'errors' in response_data:\n",
    "                        for error in response_data['errors']:\n",
    "                            error_msg = error.get('message', '')\n",
    "                            if 'ColumnValueException' in error_msg:\n",
    "                                print(f\"      ‚ùå Dropdown Error: {error_msg[:100]}...\")\n",
    "                                \n",
    "                                # Extract problematic values\n",
    "                                if color and 'not found' in error_msg.lower():\n",
    "                                    color_errors.add(color)\n",
    "                                if style and 'not found' in error_msg.lower():\n",
    "                                    style_errors.add(style)\n",
    "                except:\n",
    "                    print(f\"      ‚ö†Ô∏è  Could not parse response payload\")\n",
    "        \n",
    "        print(f\"\\nüéØ DROPDOWN VALUES CAUSING ERRORS:\")\n",
    "        if color_errors:\n",
    "            print(f\"   Colors: {list(color_errors)[:5]}\")\n",
    "        if style_errors:\n",
    "            print(f\"   Styles: {list(style_errors)[:5]}\")\n",
    "    \n",
    "    # Check for PENDING records that might have dropdown issues\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM FACT_ORDER_LIST \n",
    "        WHERE sync_state = 'PENDING' \n",
    "        AND api_status IS NULL\n",
    "    \"\"\")\n",
    "    \n",
    "    pending_count = cursor.fetchone()[0]\n",
    "    print(f\"\\nüìã Records in PENDING state (not yet attempted): {pending_count:,}\")\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "print(\"\\n‚úÖ Dropdown error analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85153e3d",
   "metadata": {},
   "source": [
    "## Customer-by-Customer Analysis\n",
    "Analyze sync status by customer for targeted processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f936dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Customer-by-Customer Sync Analysis\"\"\"\n",
    "\n",
    "with db.get_connection('orders') as conn:\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"üë• CUSTOMER-BY-CUSTOMER ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Customer sync status breakdown\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            [CUSTOMER NAME],\n",
    "            COUNT(*) as total_records,\n",
    "            SUM(CASE WHEN monday_item_id IS NOT NULL THEN 1 ELSE 0 END) as successful_syncs,\n",
    "            SUM(CASE WHEN api_status = 'ERROR' THEN 1 ELSE 0 END) as error_records,\n",
    "            SUM(CASE WHEN sync_state = 'PENDING' AND api_status IS NULL THEN 1 ELSE 0 END) as pending_records,\n",
    "            COUNT(DISTINCT group_id) as unique_groups\n",
    "        FROM FACT_ORDER_LIST\n",
    "        GROUP BY [CUSTOMER NAME]\n",
    "        ORDER BY total_records DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    print(f\"\\nüìä Customer Sync Summary ({len(results)} customers):\")\n",
    "    print(f\"{'Customer':25} {'Total':>8} {'Success':>8} {'Errors':>8} {'Pending':>8} {'Groups':>8} {'Success %':>10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for customer, total, success, errors, pending, groups in results[:10]:  # Top 10 customers\n",
    "        success_rate = (success / total * 100) if total > 0 else 0\n",
    "        print(f\"{customer[:24]:25} {total:>8,} {success:>8,} {errors:>8,} {pending:>8,} {groups:>8} {success_rate:>9.1f}%\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT [CUSTOMER NAME]) as total_customers,\n",
    "            SUM(CASE WHEN monday_item_id IS NOT NULL THEN 1 ELSE 0 END) as total_successful,\n",
    "            SUM(CASE WHEN api_status = 'ERROR' THEN 1 ELSE 0 END) as total_errors,\n",
    "            SUM(CASE WHEN sync_state = 'PENDING' AND api_status IS NULL THEN 1 ELSE 0 END) as total_pending,\n",
    "            COUNT(*) as total_records\n",
    "        FROM FACT_ORDER_LIST\n",
    "    \"\"\")\n",
    "    \n",
    "    summary = cursor.fetchone()\n",
    "    customers, successful, errors, pending, total = summary\n",
    "    \n",
    "    print(f\"\\nüìà OVERALL SUMMARY:\")\n",
    "    print(f\"   Total customers: {customers}\")\n",
    "    print(f\"   Total records: {total:,}\")\n",
    "    print(f\"   Successful syncs: {successful:,} ({successful/total*100:.1f}%)\")\n",
    "    print(f\"   Error records: {errors:,} ({errors/total*100:.1f}%)\")\n",
    "    print(f\"   Pending records: {pending:,} ({pending/total*100:.1f}%)\")\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "print(\"\\n‚úÖ Customer analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bbb055",
   "metadata": {},
   "source": [
    "## Targeted Retry Queries\n",
    "SQL queries for implementing targeted retry functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Targeted Retry Analysis - Identify retry candidates\"\"\"\n",
    "\n",
    "with db.get_connection('orders') as conn:\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"üîÅ TARGETED RETRY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Records with ERROR status (immediate retry candidates)\n",
    "    print(\"\\nüéØ IMMEDIATE RETRY CANDIDATES:\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            [CUSTOMER NAME],\n",
    "            COUNT(*) as error_count,\n",
    "            COUNT(DISTINCT record_uuid) as unique_orders\n",
    "        FROM FACT_ORDER_LIST\n",
    "        WHERE api_status = 'ERROR'\n",
    "        GROUP BY [CUSTOMER NAME]\n",
    "        ORDER BY error_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    error_results = cursor.fetchall()\n",
    "    if error_results:\n",
    "        print(f\"   {'Customer':25} {'Error Records':>15} {'Unique Orders':>15}\")\n",
    "        print(\"   \" + \"-\" * 55)\n",
    "        for customer, error_count, unique_orders in error_results:\n",
    "            print(f\"   {customer[:24]:25} {error_count:>15,} {unique_orders:>15,}\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ No records with ERROR status found\")\n",
    "    \n",
    "    # 2. Records in PENDING state (never attempted)\n",
    "    print(\"\\n‚è≥ PENDING RETRY CANDIDATES:\")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            [CUSTOMER NAME],\n",
    "            COUNT(*) as pending_count,\n",
    "            COUNT(DISTINCT record_uuid) as unique_orders\n",
    "        FROM FACT_ORDER_LIST\n",
    "        WHERE sync_state = 'PENDING' \n",
    "        AND api_status IS NULL\n",
    "        GROUP BY [CUSTOMER NAME]\n",
    "        ORDER BY pending_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    pending_results = cursor.fetchall()\n",
    "    if pending_results:\n",
    "        print(f\"   {'Customer':25} {'Pending Records':>15} {'Unique Orders':>15}\")\n",
    "        print(\"   \" + \"-\" * 55)\n",
    "        for customer, pending_count, unique_orders in pending_results[:10]:  # Top 10\n",
    "            print(f\"   {customer[:24]:25} {pending_count:>15,} {unique_orders:>15,}\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ No records in PENDING state found\")\n",
    "    \n",
    "    # 3. Generate retry SQL templates\n",
    "    print(\"\\nüõ†Ô∏è  RETRY SQL TEMPLATES:\")\n",
    "    print(\"\\n   A. Retry ERROR records for specific customer:\")\n",
    "    print(\"   ```sql\")\n",
    "    print(\"   UPDATE FACT_ORDER_LIST\")\n",
    "    print(\"   SET sync_state = 'PENDING',\")\n",
    "    print(\"       api_status = NULL,\")\n",
    "    print(\"       retry_count = COALESCE(retry_count, 0) + 1,\")\n",
    "    print(\"       updated_at = GETDATE()\")\n",
    "    print(\"   WHERE api_status = 'ERROR'\")\n",
    "    print(\"   AND [CUSTOMER NAME] = 'CUSTOMER_NAME_HERE'\")\n",
    "    print(\"   ```\")\n",
    "    \n",
    "    print(\"\\n   B. Retry all PENDING records for specific customer:\")\n",
    "    print(\"   ```sql\")\n",
    "    print(\"   SELECT record_uuid, [AAG ORDER NUMBER], [CUSTOMER NAME]\")\n",
    "    print(\"   FROM FACT_ORDER_LIST\")\n",
    "    print(\"   WHERE sync_state = 'PENDING'\")\n",
    "    print(\"   AND api_status IS NULL\")\n",
    "    print(\"   AND [CUSTOMER NAME] = 'CUSTOMER_NAME_HERE'\")\n",
    "    print(\"   ORDER BY [AAG ORDER NUMBER]\")\n",
    "    print(\"   ```\")\n",
    "    \n",
    "    print(\"\\n   C. Reset groups for customer (if group creation needed):\")\n",
    "    print(\"   ```sql\")\n",
    "    print(\"   UPDATE FACT_ORDER_LIST\")\n",
    "    print(\"   SET group_id = NULL\")\n",
    "    print(\"   WHERE [CUSTOMER NAME] = 'CUSTOMER_NAME_HERE'\")\n",
    "    print(\"   AND sync_state IN ('PENDING', 'ERROR')\")\n",
    "    print(\"   ```\")\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "print(\"\\n‚úÖ Retry analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1321b",
   "metadata": {},
   "source": [
    "## Summary Report Generator\n",
    "Generate markdown summary reports for customer sync status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate Customer Sync Summary Report in Markdown Format\"\"\"\n",
    "\n",
    "def generate_customer_summary_report(customer_name):\n",
    "    \"\"\"Generate markdown summary report for specific customer\"\"\"\n",
    "    \n",
    "    with db.get_connection('orders') as conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Customer overview\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_items,\n",
    "                SUM(CASE WHEN monday_item_id IS NOT NULL THEN 1 ELSE 0 END) as items_loaded,\n",
    "                SUM(CASE WHEN api_status = 'SUCCESS' OR monday_item_id IS NOT NULL THEN 1 ELSE 0 END) as successful,\n",
    "                SUM(CASE WHEN api_status = 'ERROR' THEN 1 ELSE 0 END) as errors,\n",
    "                SUM(CASE WHEN retry_count > 0 THEN 1 ELSE 0 END) as retries,\n",
    "                SUM(CASE WHEN sync_state = 'PENDING' AND api_status IS NULL THEN 1 ELSE 0 END) as pending,\n",
    "                COUNT(DISTINCT group_id) as groups_created,\n",
    "                COUNT(DISTINCT record_uuid) as unique_orders\n",
    "            FROM FACT_ORDER_LIST\n",
    "            WHERE [CUSTOMER NAME] = ?\n",
    "        \"\"\", (customer_name,))\n",
    "        \n",
    "        stats = cursor.fetchone()\n",
    "        total, loaded, successful, errors, retries, pending, groups, orders = stats\n",
    "        \n",
    "        # Group breakdown\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT \n",
    "                COALESCE(group_name, 'NO GROUP') as group_name,\n",
    "                COUNT(*) as total_items,\n",
    "                SUM(CASE WHEN monday_item_id IS NOT NULL THEN 1 ELSE 0 END) as items_loaded,\n",
    "                SUM(CASE WHEN api_status = 'ERROR' THEN 1 ELSE 0 END) as errors,\n",
    "                SUM(CASE WHEN sync_state = 'PENDING' AND api_status IS NULL THEN 1 ELSE 0 END) as pending\n",
    "            FROM FACT_ORDER_LIST\n",
    "            WHERE [CUSTOMER NAME] = ?\n",
    "            GROUP BY group_name\n",
    "            ORDER BY total_items DESC\n",
    "        \"\"\", (customer_name,))\n",
    "        \n",
    "        group_stats = cursor.fetchall()\n",
    "        \n",
    "        # Critical issues\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT TOP 5\n",
    "                [AAG ORDER NUMBER],\n",
    "                [CUSTOMER COLOUR DESCRIPTION],\n",
    "                [CUSTOMER STYLE],\n",
    "                api_status,\n",
    "                sync_error_message\n",
    "            FROM FACT_ORDER_LIST\n",
    "            WHERE [CUSTOMER NAME] = ?\n",
    "            AND api_status = 'ERROR'\n",
    "            ORDER BY sync_attempted_at DESC\n",
    "        \"\"\", (customer_name,))\n",
    "        \n",
    "        critical_issues = cursor.fetchall()\n",
    "        \n",
    "        cursor.close()\n",
    "    \n",
    "    # Generate markdown report\n",
    "    report = []\n",
    "    report.append(f\"# {customer_name}\")\n",
    "    report.append(f\"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Summary table\n",
    "    success_rate = (successful / total * 100) if total > 0 else 0\n",
    "    overall_result = \"‚úÖ Success\" if success_rate >= 95 else \"‚ö†Ô∏è Needs Attention\" if success_rate >= 80 else \"‚ùå Critical\"\n",
    "    \n",
    "    report.append(\"## Summary\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"| Metric | Count | Percentage |\")\n",
    "    report.append(\"|--------|-------|------------|\")\n",
    "    report.append(f\"| Total Items | {total:,} | 100.0% |\")\n",
    "    report.append(f\"| Items Loaded | {loaded:,} | {(loaded/total*100):.1f}% |\")\n",
    "    report.append(f\"| Successful | {successful:,} | {(successful/total*100):.1f}% |\")\n",
    "    report.append(f\"| Errors | {errors:,} | {(errors/total*100):.1f}% |\")\n",
    "    report.append(f\"| Retries | {retries:,} | {(retries/total*100):.1f}% |\")\n",
    "    report.append(f\"| Pending | {pending:,} | {(pending/total*100):.1f}% |\")\n",
    "    report.append(f\"| Groups Created | {groups} | - |\")\n",
    "    report.append(f\"| **Overall Result** | **{overall_result}** | **{success_rate:.1f}%** |\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Critical items section\n",
    "    report.append(\"### Critical Items to Review\")\n",
    "    report.append(\"\")\n",
    "    if critical_issues:\n",
    "        report.append(\"| Order Number | Color | Style | Status | Error Message |\")\n",
    "        report.append(\"|--------------|-------|-------|--------|---------------|\")\n",
    "        for order, color, style, status, error_msg in critical_issues:\n",
    "            error_short = (error_msg or \"\")[:50] + \"...\" if error_msg and len(error_msg) > 50 else (error_msg or \"\")\n",
    "            report.append(f\"| {order} | {color or 'N/A'} | {style or 'N/A'} | {status} | {error_short} |\")\n",
    "    else:\n",
    "        report.append(\"‚úÖ No critical issues found\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Group breakdown\n",
    "    report.append(\"### Summary by Group\")\n",
    "    report.append(\"\")\n",
    "    if group_stats:\n",
    "        report.append(\"| Group Name | Total Items | Loaded | Errors | Pending | Success Rate |\")\n",
    "        report.append(\"|------------|-------------|--------|--------|---------|--------------|\")\n",
    "        for group_name, total_items, loaded_items, group_errors, group_pending in group_stats:\n",
    "            group_success_rate = (loaded_items / total_items * 100) if total_items > 0 else 0\n",
    "            report.append(f\"| {group_name[:20]} | {total_items:,} | {loaded_items:,} | {group_errors:,} | {group_pending:,} | {group_success_rate:.1f}% |\")\n",
    "    else:\n",
    "        report.append(\"‚ö†Ô∏è No group data available\")\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "# Generate sample report for GREYSON\n",
    "print(\"üìù SAMPLE CUSTOMER SUMMARY REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sample_customer = \"GREYSON\"\n",
    "report = generate_customer_summary_report(sample_customer)\n",
    "print(report)\n",
    "\n",
    "print(\"\\nüíæ Report generation function ready for production use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5810438d",
   "metadata": {},
   "source": [
    "## TOML Configuration Analysis\n",
    "Review current TOML settings and dropdown configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacbaf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TOML Configuration Review\"\"\"\n",
    "\n",
    "import toml\n",
    "\n",
    "print(\"‚öôÔ∏è  TOML CONFIGURATION REVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load TOML configuration\n",
    "toml_path = repo_root / \"configs\" / \"pipelines\" / \"sync_order_list.toml\"\n",
    "with open(toml_path, 'r') as f:\n",
    "    config = toml.load(f)\n",
    "\n",
    "print(f\"üìÑ Configuration file: {toml_path}\")\n",
    "print(f\"üîß Current phase: {config.get('phase', {}).get('current', 'Unknown')}\")\n",
    "\n",
    "# Dropdown configuration analysis\n",
    "print(\"\\nüéØ DROPDOWN CONFIGURATION ANALYSIS:\")\n",
    "\n",
    "# Production environment dropdown settings\n",
    "prod_headers = config.get('monday', {}).get('production', {}).get('headers', {}).get('create_labels_if_missing', {})\n",
    "if prod_headers:\n",
    "    print(\"\\n   Production Environment (Board 9609317401):\")\n",
    "    critical_fields = ['dropdown_mkr5677f', 'dropdown_mkr5tgaa', 'dropdown_mkr58de6', 'dropdown_mkr5rgs6']\n",
    "    for field in critical_fields:\n",
    "        value = prod_headers.get(field, 'NOT SET')\n",
    "        status = \"‚úÖ ENABLED\" if value else \"‚ùå DISABLED\" if value is False else \"‚ö†Ô∏è NOT SET\"\n",
    "        field_name = {\n",
    "            'dropdown_mkr5677f': 'COLOR',\n",
    "            'dropdown_mkr5tgaa': 'STYLE', \n",
    "            'dropdown_mkr58de6': 'AAG SEASON',\n",
    "            'dropdown_mkr5rgs6': 'CUSTOMER SEASON'\n",
    "        }.get(field, field)\n",
    "        print(f\"      {field_name:15} ({field}): {status}\")\n",
    "\n",
    "# Development environment dropdown settings\n",
    "dev_headers = config.get('monday', {}).get('development', {}).get('headers', {}).get('create_labels_if_missing', {})\n",
    "if dev_headers:\n",
    "    print(\"\\n   Development Environment (Board 9200517329):\")\n",
    "    critical_fields = ['dropdown_mkp4kmtp']  # CUSTOMER SEASON\n",
    "    for field in critical_fields:\n",
    "        value = dev_headers.get(field, 'NOT SET')\n",
    "        status = \"‚úÖ ENABLED\" if value else \"‚ùå DISABLED\" if value is False else \"‚ö†Ô∏è NOT SET\"\n",
    "        field_name = 'CUSTOMER SEASON'\n",
    "        print(f\"      {field_name:15} ({field}): {status}\")\n",
    "\n",
    "print(\"\\nüéØ CONFIGURATION RECOMMENDATIONS:\")\n",
    "print(\"   1. ‚úÖ COLOR (dropdown_mkr5677f) = true - GOOD for handling 'ANTIQUE MOSS' errors\")\n",
    "print(\"   2. ‚úÖ STYLE (dropdown_mkr5tgaa) = true - GOOD for handling 'WT01087' errors\")\n",
    "print(\"   3. ‚úÖ AAG SEASON (dropdown_mkr58de6) = true - GOOD for season handling\")\n",
    "print(\"   4. ‚úÖ CUSTOMER SEASON (dropdown_mkr5rgs6) = true - GOOD for customer season handling\")\n",
    "\n",
    "print(\"\\n‚úÖ TOML configuration review complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9bb910",
   "metadata": {},
   "source": [
    "## Action Items & Next Steps\n",
    "Based on diagnostic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Action Items Summary\"\"\"\n",
    "\n",
    "print(\"üìã ACTION ITEMS & NEXT STEPS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüö® CRITICAL ISSUES TO ADDRESS:\")\n",
    "print(\"\\n1. **API Logging Issue**:\")\n",
    "print(\"   - Only 12 records in ORDER_LIST_API_LOG vs 1,111 successful API calls\")\n",
    "print(\"   - APILoggingArchiver not being called properly in sync_engine.py\")\n",
    "print(\"   - Need to fix API logging archiver integration\")\n",
    "\n",
    "print(\"\\n2. **Dropdown Validation Errors**:\")\n",
    "print(\"   - ‚úÖ TOML updated to enable auto-creation for problematic fields\")\n",
    "print(\"   - COLOR and STYLE dropdowns now set to create_labels_if_missing = true\")\n",
    "print(\"   - Ready for retry with updated configuration\")\n",
    "\n",
    "print(\"\\n3. **Enhanced Error Logging**:\")\n",
    "print(\"   - Implement record_uuid-based logging instead of full payloads\")\n",
    "print(\"   - Focus on essential metrics: customer, order, error type, retry count\")\n",
    "print(\"   - Create customer-focused markdown summary reports\")\n",
    "\n",
    "print(\"\\nüéØ IMPLEMENTATION PRIORITIES:\")\n",
    "print(\"\\n1. **Customer-by-Customer Processing**:\")\n",
    "print(\"   - Load ONE customer at a time\")\n",
    "print(\"   - Create missing groups for customer first\")\n",
    "print(\"   - Load all items (subitems optional)\")\n",
    "print(\"   - Generate markdown summary report\")\n",
    "\n",
    "print(\"\\n2. **Targeted Retry Functionality**:\")\n",
    "print(\"   - Build retry mechanism for api_status = 'ERROR' records\")\n",
    "print(\"   - Handle sync_state = 'PENDING' records\")\n",
    "print(\"   - Implement exponential backoff for retries\")\n",
    "\n",
    "print(\"\\n3. **Improved Reporting**:\")\n",
    "print(\"   - Markdown summary reports per customer\")\n",
    "print(\"   - Success/error metrics by group\")\n",
    "print(\"   - Critical issues identification\")\n",
    "\n",
    "print(\"\\nüíæ DIAGNOSTIC NOTEBOOK READY:\")\n",
    "print(\"   - All diagnostic queries available for reuse\")\n",
    "print(\"   - Customer analysis functions ready\")\n",
    "print(\"   - Retry query templates prepared\")\n",
    "print(\"   - Summary report generator functional\")\n",
    "\n",
    "print(\"\\n‚úÖ Diagnostic analysis complete - Ready for implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed64d1",
   "metadata": {},
   "source": [
    "## Pipeline Setup for Empty FACT_ORDER_LIST\n",
    "**Scenario**: Starting with empty FACT_ORDER_LIST and swp_ORDER_LIST_SYNC with sync_state = 'NEW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b391dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß PIPELINE SETUP DIAGNOSTICS\n",
      "==================================================\n",
      "\n",
      "üìä Current Record Counts:\n",
      "   swp_ORDER_LIST_SYNC: 7,405 records\n",
      "   FACT_ORDER_LIST: 7,405 records\n",
      "\n",
      "üîç FACT_ORDER_LIST Sync States:\n",
      "   PENDING      | INSERT   |  7,405 records\n",
      "\n",
      "üéØ Records Ready for Sync:\n",
      "   PENDING + INSERT: 7,405 records\n",
      "\n",
      "üõ†Ô∏è  PIPELINE SETUP READY\n",
      "   Use the setup commands below to prepare for sync...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pipeline Setup Diagnostics and Fix\"\"\"\n",
    "\n",
    "with db.get_connection('orders') as conn:\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"üîß PIPELINE SETUP DIAGNOSTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Current state analysis\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM swp_ORDER_LIST_SYNC\")\n",
    "    swp_count = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute(\"SELECT COUNT(*) FROM FACT_ORDER_LIST\")\n",
    "    fact_count = cursor.fetchone()[0]\n",
    "    \n",
    "    print(f\"\\nüìä Current Record Counts:\")\n",
    "    print(f\"   swp_ORDER_LIST_SYNC: {swp_count:,} records\")\n",
    "    print(f\"   FACT_ORDER_LIST: {fact_count:,} records\")\n",
    "    \n",
    "    # Check sync states in FACT_ORDER_LIST\n",
    "    if fact_count > 0:\n",
    "        print(f\"\\nüîç FACT_ORDER_LIST Sync States:\")\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT \n",
    "                COALESCE(sync_state, 'NULL') as state,\n",
    "                COALESCE(action_type, 'NULL') as action,\n",
    "                COUNT(*) as count\n",
    "            FROM FACT_ORDER_LIST \n",
    "            GROUP BY sync_state, action_type\n",
    "            ORDER BY count DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        for state, action, count in cursor.fetchall():\n",
    "            print(f\"   {state:12} | {action:8} | {count:>6,} records\")\n",
    "        \n",
    "        # Check for PENDING records ready for sync\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT COUNT(*) \n",
    "            FROM FACT_ORDER_LIST \n",
    "            WHERE sync_state = 'PENDING' AND action_type = 'INSERT'\n",
    "        \"\"\")\n",
    "        pending_ready = cursor.fetchone()[0]\n",
    "        \n",
    "        print(f\"\\nüéØ Records Ready for Sync:\")\n",
    "        print(f\"   PENDING + INSERT: {pending_ready:,} records\")\n",
    "        \n",
    "        if pending_ready == 0:\n",
    "            print(\"\\n‚ö†Ô∏è  ISSUE IDENTIFIED: No records in PENDING + INSERT state!\")\n",
    "            print(\"   The enhanced CLI looks for records with:\")\n",
    "            print(\"   - sync_state = 'PENDING'\")\n",
    "            print(\"   - action_type = 'INSERT'\")\n",
    "            \n",
    "            # Check what states we actually have\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT TOP 5\n",
    "                    [AAG ORDER NUMBER],\n",
    "                    [CUSTOMER NAME],\n",
    "                    sync_state,\n",
    "                    action_type,\n",
    "                    monday_item_id\n",
    "                FROM FACT_ORDER_LIST\n",
    "                ORDER BY [CUSTOMER NAME], [AAG ORDER NUMBER]\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"\\nüîç Sample Records (showing current states):\")\n",
    "            for aag, customer, state, action, item_id in cursor.fetchall():\n",
    "                print(f\"   {aag:15} | {customer[:20]:20} | {state or 'NULL':12} | {action or 'NULL':8} | {item_id or 'NULL'}\")\n",
    "    \n",
    "    cursor.close()\n",
    "\n",
    "print(\"\\nüõ†Ô∏è  PIPELINE SETUP READY\")\n",
    "print(\"   Use the setup commands below to prepare for sync...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13079f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pipeline Setup Commands\"\"\"\n",
    "\n",
    "print(\"üîß PIPELINE SETUP COMMANDS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüìã STEP 1: Set up FACT_ORDER_LIST for sync\")\n",
    "print(\"   Execute this SQL to prepare records for sync:\")\n",
    "print()\n",
    "print(\"   ```sql\")\n",
    "print(\"   -- Set all records to PENDING + INSERT state for sync\")\n",
    "print(\"   UPDATE FACT_ORDER_LIST\")\n",
    "print(\"   SET sync_state = 'PENDING',\")\n",
    "print(\"       action_type = 'INSERT',\")\n",
    "print(\"       updated_at = GETDATE()\")\n",
    "print(\"   WHERE sync_state IS NULL OR sync_state = 'NEW'\")\n",
    "print(\"   ```\")\n",
    "\n",
    "print(\"\\nüìã STEP 2: Filter to specific customer (optional)\")\n",
    "print(\"   For GREYSON only:\")\n",
    "print()\n",
    "print(\"   ```sql\")\n",
    "print(\"   -- Set only GREYSON records to PENDING for testing\")\n",
    "print(\"   UPDATE FACT_ORDER_LIST\")\n",
    "print(\"   SET sync_state = 'COMPLETED',  -- Skip other customers\")\n",
    "print(\"       action_type = 'NONE'\")\n",
    "print(\"   WHERE [CUSTOMER NAME] != 'GREYSON'\")\n",
    "print(\"   ```\")\n",
    "\n",
    "print(\"\\nüìã STEP 3: Reset groups (if needed)\")\n",
    "print(\"   If you want to recreate groups:\")\n",
    "print()\n",
    "print(\"   ```sql\")\n",
    "print(\"   -- Clear group assignments to force recreation\")\n",
    "print(\"   UPDATE FACT_ORDER_LIST\")\n",
    "print(\"   SET group_id = NULL\")\n",
    "print(\"   WHERE [CUSTOMER NAME] = 'GREYSON'\")\n",
    "print(\"   AND sync_state = 'PENDING'\")\n",
    "print(\"   ```\")\n",
    "\n",
    "print(\"\\nüöÄ STEP 4: Run Enhanced CLI\")\n",
    "print(\"   After setting up the data, run:\")\n",
    "print()\n",
    "print(\"   ```bash\")\n",
    "print(\"   # For GREYSON customer with 10 record limit\")\n",
    "print(\"   python -m src.pipelines.sync_order_list.cli sync --execute --limit 10 --retry-errors --generate-report --customer 'GREYSON'\")\n",
    "print()\n",
    "print(\"   # For all customers (no filter)\")\n",
    "print(\"   python -m src.pipelines.sync_order_list.cli sync --execute --limit 10 --retry-errors --generate-report\")\n",
    "print(\"   ```\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup commands ready - Execute SQL first, then run CLI\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
